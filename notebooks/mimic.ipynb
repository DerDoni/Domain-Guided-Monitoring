{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd09d90ef2e2544a65949a5382aa665e8a895142ccb15d506742792c571feba52d3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_icd9(dxStr):\n",
    "    if dxStr.startswith('E'):\n",
    "        if len(dxStr) > 4: return dxStr[:4] + '.' + dxStr[4:]\n",
    "        else: return dxStr\n",
    "    else:\n",
    "        if len(dxStr) > 3: return dxStr[:3] + '.' + dxStr[3:]\n",
    "        else: return dxStr\n",
    "    \n",
    "def convert_to_3digit_icd9(dxStr):\n",
    "    if dxStr.startswith('E'):\n",
    "        if len(dxStr) > 4: return dxStr[:4]\n",
    "        else: return dxStr\n",
    "    else:\n",
    "        if len(dxStr) > 3: return dxStr[:3]\n",
    "        else: return dxStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admission_file = Path('../data/ADMISSIONS.csv')\n",
    "diagnosis_file = Path('../data/DIAGNOSES_ICD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions_df = pd.read_csv(admission_file)\n",
    "admissions_df['admittime']= pd.to_datetime(admissions_df['admittime'])\n",
    "admissions_df['dischtime']= pd.to_datetime(admissions_df['dischtime'])\n",
    "admissions_df['deathtime']= pd.to_datetime(admissions_df['deathtime'])\n",
    "admissions_df['edregtime']= pd.to_datetime(admissions_df['edregtime'])\n",
    "admissions_df['edouttime']= pd.to_datetime(admissions_df['edouttime'])\n",
    "admissions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_df = pd.read_csv(diagnosis_file)\n",
    "diagnosis_df['icd9_code_converted'] = diagnosis_df['icd9_code'].apply(convert_to_icd9)\n",
    "diagnosis_df['icd9_code_converted_3digits'] = diagnosis_df['icd9_code'].apply(convert_to_3digit_icd9)\n",
    "diagnosis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_per_admission = diagnosis_df.groupby('hadm_id').agg({\n",
    "    'icd9_code': lambda x: list(x),\n",
    "    'icd9_code_converted': lambda x: list(x),\n",
    "    'icd9_code_converted_3digits': lambda x: list(x),\n",
    "})\n",
    "combined_df = pd.merge(admissions_df, codes_per_admission, on=['hadm_id'])\n",
    "admissions_per_subject = combined_df.groupby('subject_id').agg({\n",
    "    'hadm_id': lambda x: list(x),\n",
    "    'admittime': lambda x: list(x),\n",
    "    'diagnosis': lambda x: list(x),\n",
    "    'icd9_code': lambda x: list(x),\n",
    "    'icd9_code_converted': lambda x: list(x),\n",
    "    'icd9_code_converted_3digits': lambda x: list(x),\n",
    "})\n",
    "admissions_per_subject['num_admissions'] = admissions_per_subject['hadm_id'].apply(len)\n",
    "admissions_per_subject.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = admissions_per_subject[admissions_per_subject['num_admissions'] >= 2]\n",
    "relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symptoms = list(set([item for sublist in relevant_data['icd9_code_converted_3digits'].agg(lambda x: [item for sublist in x for item in sublist]).tolist() for item in sublist]))\n",
    "vocab = {}\n",
    "index = 0\n",
    "for symptom in all_symptoms:\n",
    "    vocab[symptom] = index\n",
    "    index = index+1\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = relevant_data['icd9_code_converted_3digits'].apply(len).max()\n",
    "max_symptoms_per_sequence = relevant_data['icd9_code_converted_3digits'].apply(lambda x: sum([len(y) for y in x])).max()\n",
    "train_sequences, test_sequences = train_test_split(\n",
    "    relevant_data['icd9_code_converted_3digits'], \n",
    "    test_size=0.1, \n",
    "    random_state=12345)\n",
    "train_sequences.tolist()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence):\r\n",
    "    splitted = []\r\n",
    "    for split_index in range(1, len(sequence)):\r\n",
    "        splitted.append({\r\n",
    "            'x': sequence[0:split_index],\r\n",
    "            'y': sequence[split_index], \r\n",
    "        })\r\n",
    "\r\n",
    "    return splitted\r\n",
    "\r\n",
    "def split_sequences(sequences):\r\n",
    "    splitted_sequences = []\r\n",
    "    for sequence in sequences:\r\n",
    "        splitted_sequences.extend(split_sequence(sequence))\r\n",
    "\r\n",
    "    return splitted_sequences\r\n",
    "\r\n",
    "def transform_symptoms(symptoms, vocab):\r\n",
    "    symptom_vec = np.zeros(len(vocab))\r\n",
    "    for symptom in symptoms:\r\n",
    "        symptom_vec[vocab[symptom]] = 1\r\n",
    "    return tf.convert_to_tensor(symptom_vec)\r\n",
    "\r\n",
    "def translate_and_pad_x_flat(splitted, vocab, max_sequence_length):\r\n",
    "    splitted['x_vecs'] = []\r\n",
    "    for i in range(max_sequence_length - len(splitted['x'])):\r\n",
    "        splitted['x_vecs'].append(transform_symptoms([], vocab))\r\n",
    "    for x in splitted['x']:\r\n",
    "        splitted['x_vecs'].append(transform_symptoms(x, vocab))\r\n",
    "    splitted['x_vecs_stacked'] = tf.stack(splitted['x_vecs'])\r\n",
    "\r\n",
    "def translate_and_pad_x_wide(splitted, vocab, max_symptoms_per_sequence):\r\n",
    "    splitted['x_vecs'] = []\r\n",
    "    all_symptoms = [symptom for x in splitted['x'] for symptom in x]\r\n",
    "    for i in range(max_symptoms_per_sequence - len(all_symptoms)):\r\n",
    "        splitted['x_vecs'].append(transform_symptoms([], vocab))\r\n",
    "    for symptom in all_symptoms:\r\n",
    "        splitted['x_vecs'].append(transform_symptoms([symptom], vocab))\r\n",
    "    splitted['x_vecs_stacked'] = tf.stack(splitted['x_vecs'])\r\n",
    "\r\n",
    "def translate_and_pad(splitted, vocab, max_sequence_length, max_symptoms_per_sequence, flat):\r\n",
    "    splitted['y_vec'] = transform_symptoms(splitted['y'], vocab)\r\n",
    "    if flat:\r\n",
    "        translate_and_pad_x_flat(splitted, vocab, max_sequence_length)\r\n",
    "    else:\r\n",
    "        translate_and_pad_x_wide(splitted, vocab, max_symptoms_per_sequence)\r\n",
    "    \r\n",
    "\r\n",
    "def transform_sequences(sequences, vocab, max_sequence_length, max_symptoms_per_sequence, flat=True):\r\n",
    "    splitted_sequences = split_sequences(sequences)\r\n",
    "    for splitted in splitted_sequences:\r\n",
    "        translate_and_pad(splitted, vocab, max_sequence_length, max_symptoms_per_sequence, flat)\r\n",
    "\r\n",
    "    return splitted_sequences\r\n",
    "\r\n",
    "\r\n",
    "transformed_5 = transform_sequences(train_sequences.tolist()[5:6], vocab, max_sequence_length, max_symptoms_per_sequence, flat=False)\r\n",
    "tf.stack([[transformed['y_vec']] for transformed in transformed_5])\r\n",
    "tf.stack([transformed['x_vecs_stacked'] for transformed in transformed_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed_flat = transform_sequences(train_sequences.tolist(), vocab, max_sequence_length, max_symptoms_per_sequence, flat=True)\n",
    "test_transformed_flat = transform_sequences(test_sequences.tolist(), vocab, max_sequence_length, max_symptoms_per_sequence, flat=True)\n",
    "\n",
    "train_x_flat = tf.stack([transformed['x_vecs_stacked'] for transformed in train_transformed_flat])\n",
    "train_y_flat = tf.stack([[transformed['y_vec']] for transformed in train_transformed_flat])\n",
    "test_x_flat = tf.stack([transformed['x_vecs_stacked'] for transformed in test_transformed_flat])\n",
    "test_y_flat = tf.stack([[transformed['y_vec']] for transformed in test_transformed_flat])\n",
    "\n",
    "print(train_x_flat.shape)\n",
    "print(train_y_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed_wide = transform_sequences(train_sequences.tolist(), vocab, max_sequence_length, max_symptoms_per_sequence, flat=False)\n",
    "test_transformed_wide = transform_sequences(test_sequences.tolist(), vocab, max_sequence_length, max_symptoms_per_sequence, flat=False)\n",
    "\n",
    "train_x_wide = tf.stack([transformed['x_vecs_stacked'] for transformed in train_transformed_wide])\n",
    "train_y_wide = tf.stack([[transformed['y_vec']] for transformed in train_transformed_wide])\n",
    "test_x_wide = tf.stack([transformed['x_vecs_stacked'] for transformed in test_transformed_wide])\n",
    "test_y_wide = tf.stack([[transformed['y_vec']] for transformed in test_transformed_wide])\n",
    "\n",
    "print(train_x_wide.shape)\n",
    "print(train_y_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_flat = tf.keras.layers.Input(shape=(max_sequence_length,len(vocab)))\n",
    "emb_layer_flat = tf.keras.layers.Dense(64)\n",
    "lstm_model_flat = tf.keras.models.Sequential([\n",
    "    input_layer_flat,\n",
    "    emb_layer_flat,\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Conv1D(filters=32,\n",
    "    #                       kernel_size=(3,),\n",
    "    #                       activation='relu'),\n",
    "    tf.keras.layers.Dense(len(vocab), activation='relu'),\n",
    "])\n",
    "lstm_model_flat.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam())\n",
    "lstm_model_flat.fit(x=train_x_flat, y=train_y_flat, epochs=100)\n",
    "\n",
    "emb_model_flat = tf.keras.models.Sequential([\n",
    "    input_layer_flat,\n",
    "    emb_layer_flat,\n",
    "])\n",
    "print(emb_model_flat.predict(train_x_flat).shape)\n",
    "\n",
    "lstm_model_flat.evaluate(test_x_flat, test_y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_wide = tf.keras.layers.Input(shape=(max_symptoms_per_sequence,len(vocab)))\n",
    "emb_layer_wide = tf.keras.layers.Dense(64)\n",
    "lstm_model_wide = tf.keras.models.Sequential([\n",
    "    input_layer_wide,\n",
    "    emb_layer_wide,\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    tf.keras.layers.Dense(len(vocab), activation='relu'),\n",
    "])\n",
    "lstm_model_wide.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.optimizers.Adam())\n",
    "lstm_model_wide.fit(x=train_x_wide, y=train_y_wide, epochs=100)\n",
    "\n",
    "emb_model_wide = tf.keras.models.Sequential([\n",
    "    input_layer_wide,\n",
    "    emb_layer_wide,\n",
    "])\n",
    "print(emb_model_wide.predict(train_x_wide).shape)\n",
    "\n",
    "lstm_model_wide.predict(test_x_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lstm_model_wide.predict(test_x_wide)[0]\n",
    "np.argwhere(prediction > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(test_y_wide[1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x, vocab[x]) for x in vocab.keys() if vocab[x] in [7, 26]]"
   ]
  }
 ]
}