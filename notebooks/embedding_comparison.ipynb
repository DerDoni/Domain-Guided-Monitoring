{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "interpreter": {
   "hash": "9d90ef2e2544a65949a5382aa665e8a895142ccb15d506742792c571feba52d3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from pathlib import Path\r\n",
    "from sklearn.cross_decomposition import CCA\r\n",
    "from scipy.spatial.distance import pdist, squareform\r\n",
    "import numpy as np\r\n",
    "import scipy.linalg as linalg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%run utils/mlflow_query.py\r\n",
    "%run utils/loading.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow_helper = MlflowHelper(pkl_file=Path(\"mlflow_run_df.pkl\"))\r\n",
    "#mlflow_helper.query_all_runs(query_metrics=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GCCA:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def fit(self, views):\r\n",
    "        dims = [view.shape[1] for view in views]\r\n",
    "        concat = np.concatenate(views, axis = 1)\r\n",
    "        self.mean = concat.mean(axis = 0)\r\n",
    "        cov = np.cov(concat.T)\r\n",
    "\r\n",
    "        mask = linalg.block_diag(*[np.ones((dim, dim), bool) for dim in dims])\r\n",
    "\r\n",
    "        eigvals, eigvecs = linalg.eigh(cov * np.invert(mask), cov * mask)\r\n",
    "        tol = abs(eigvals.real).max() * len(eigvals) * np.finfo(float).eps\r\n",
    "\r\n",
    "        self.theta = eigvecs.real[:, np.greater(abs(eigvals.real), tol)]\r\n",
    "    \r\n",
    "    def transform(self, views):\r\n",
    "        concat = np.concatenate(views, axis = 1)\r\n",
    "        return (concat - self.mean).dot(self.theta)\r\n",
    "\r\n",
    "    def transform_as_list(self, views):\r\n",
    "        dims = [view.shape[1] for view in views]\r\n",
    "        outputs = []\r\n",
    "\r\n",
    "        slc = slice(0,0)\r\n",
    "        for i, dim in enumerate(dims):\r\n",
    "            slc = slice(slc.stop, slc.stop + dim)\r\n",
    "            outputs.append((views[i] - self.mean[slc]).dot(self.theta[slc]))\r\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load_vec_dict(run_id, local_mlflow_dir):\r\n",
    "    run_vec_path = Path(local_mlflow_dir + run_id + '/artifacts/vecs.tsv')\r\n",
    "    run_meta_path = Path(local_mlflow_dir + run_id + '/artifacts/meta.tsv')\r\n",
    "    if not run_vec_path.exists() or not run_meta_path.exists():\r\n",
    "        return None\r\n",
    "\r\n",
    "    vec_np = np.genfromtxt(fname=run_vec_path, delimiter=\"\\t\") # shape: (num_features, embedding_dim)\r\n",
    "    meta_df = pd.read_csv(run_meta_path, sep='\\t', header=None, names=['name'], dtype=str)\r\n",
    "    return {\r\n",
    "        meta_df.loc[idx, 'name']:vec_np[idx,:] for idx in range(len(meta_df))\r\n",
    "    }\r\n",
    "\r\n",
    "def calculate_correlation(vectors, other_vectors, dims=100, print_diag=False):\r\n",
    "    corr_matrix = np.corrcoef(vectors, other_vectors, rowvar=False)\r\n",
    "    corr_matrix = corr_matrix[dims:, :dims]\r\n",
    "    diag = np.diag(corr_matrix)\r\n",
    "    if print_diag: print(diag)\r\n",
    "    return np.mean(diag)\r\n",
    "\r\n",
    "def pretty_print_result_dict(cca_correlation_dict):\r\n",
    "    for from_corr, to_corr_dict in cca_correlation_dict.items():\r\n",
    "        print(from_corr + '\\n')\r\n",
    "        for to_corr, corr_values in to_corr_dict.items():\r\n",
    "            print('\\t' + to_corr + '\\n\\t -> before:' + str(corr_values[0]) + '\\n\\t -> after:' + str(corr_values[1]) + '\\n\\t -> fitting score:' + str(corr_values[2]))\r\n",
    "\r\n",
    "def calculate_cca_corr_between(run_id, other_run_id, relevant_run_ids, dims=100):\r\n",
    "    if run_id not in relevant_run_ids or other_run_id not in relevant_run_ids:\r\n",
    "        return None\r\n",
    "    \r\n",
    "    run_mlflow_dir = Path(local_mlflow_dir + run_id)\r\n",
    "    other_run_mlflow_dir = Path(local_mlflow_dir + other_run_id)\r\n",
    "    if not run_mlflow_dir.is_dir() or not other_run_mlflow_dir.is_dir():\r\n",
    "        return None\r\n",
    "\r\n",
    "    vec_dict = load_vec_dict(run_id)\r\n",
    "    other_vec_dict = load_vec_dict(other_run_id)\r\n",
    "    if vec_dict is None or other_vec_dict is None:\r\n",
    "        return None\r\n",
    "\r\n",
    "    shared_meta = [\r\n",
    "        x for x in vec_dict if x in other_vec_dict and not x.endswith('_hidden') and not x.endswith('_base')\r\n",
    "    ]\r\n",
    "\r\n",
    "    vectors = np.array([vec_dict[x] for x in shared_meta])\r\n",
    "    other_vectors = np.array([other_vec_dict[x] for x in shared_meta])\r\n",
    "    corr_before = calculate_correlation(vectors, other_vectors, dims=300)\r\n",
    "\r\n",
    "    print('Calculating CCA...')\r\n",
    "    cca = CCA(n_components=dims, max_iter=5000)\r\n",
    "    cca.fit(vectors, other_vectors)\r\n",
    "    vectors_t, other_vectors_t = cca.transform(\r\n",
    "        vectors, other_vectors\r\n",
    "    )\r\n",
    "\r\n",
    "    print('Calculating correlation coefficient...')\r\n",
    "    corr_after = calculate_correlation(vectors_t, other_vectors_t, dims=dims, print_diag=True)\r\n",
    "    return (corr_before, corr_after, cca.score(vectors, other_vectors))\r\n",
    "\r\n",
    "def calculate_gcca_corr_between(run_id, other_run_id, relevant_run_ids, dims=100, local_mlflow_dir='../gsim01/mlruns/' + experiment.experiment_id + '/'):\r\n",
    "    if run_id not in relevant_run_ids or other_run_id not in relevant_run_ids:\r\n",
    "        return None\r\n",
    "    \r\n",
    "    run_mlflow_dir = Path(local_mlflow_dir + run_id)\r\n",
    "    other_run_mlflow_dir = Path(local_mlflow_dir + other_run_id)\r\n",
    "    if not run_mlflow_dir.is_dir() or not other_run_mlflow_dir.is_dir():\r\n",
    "        return None\r\n",
    "\r\n",
    "    vec_dict = load_vec_dict(run_id)\r\n",
    "    other_vec_dict = load_vec_dict(other_run_id)\r\n",
    "    if vec_dict is None or other_vec_dict is None:\r\n",
    "        return None\r\n",
    "\r\n",
    "    shared_meta = [\r\n",
    "        x for x in vec_dict if x in other_vec_dict and not x.endswith('_hidden') and not x.endswith('_base')\r\n",
    "    ]\r\n",
    "\r\n",
    "    vectors = np.array([vec_dict[x] for x in shared_meta])\r\n",
    "    other_vectors = np.array([other_vec_dict[x] for x in shared_meta])\r\n",
    "    corr_before = calculate_correlation(vectors, other_vectors, dims=300)\r\n",
    "\r\n",
    "    #print('Calculating GCCA...')\r\n",
    "    gcca = GCCA()\r\n",
    "    gcca.fit([vectors, other_vectors])\r\n",
    "    transform_l = gcca.transform_as_list((vectors, other_vectors))\r\n",
    "    # gcca computes positive and negative correlations (eigenvalues), sorted in ascending order.\r\n",
    "    # We are only interested in the positive portion\r\n",
    "    vectors_t = transform_l[0][:,dims:]\r\n",
    "    other_vectors_t = transform_l[1][:,dims:]\r\n",
    "\r\n",
    "    #print('Calculating correlation coefficient...')\r\n",
    "    corr_after = calculate_correlation(vectors_t, other_vectors_t, dims=dims)\r\n",
    "    return (corr_before, corr_after, np.nan)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_all_correlations(runs, relevant_run_ids, correlation_map={}, local_mlflow_dir='../gsim01/mlruns/' + experiment.experiment_id + '/', dims=300):\r\n",
    "    number_correlations = len(relevant_run_ids) * len(relevant_run_ids)\r\n",
    "    with tqdm(total=number_correlations, desc='Calculating embedding gcca correlations per run') as pbar:\r\n",
    "        for run in runs:\r\n",
    "            if not 'model_type' in run.data.tags:\r\n",
    "                continue\r\n",
    "            run_id = run.info.run_id\r\n",
    "            if run_id not in relevant_run_ids:\r\n",
    "                continue\r\n",
    "            suffix = '_{model}_{trainable}{noise}_{run_id}'.format(\r\n",
    "                model=run.data.tags['model_type'],\r\n",
    "                trainable=run.data.params['ModelConfigbase_feature_embeddings_trainable'],\r\n",
    "                noise=(run.data.tags['noise_type'] + '_') \r\n",
    "                    if 'noise_type' in run.data.tags \r\n",
    "                        and not str(run.data.tags['noise_type']).lower() == 'nan' \r\n",
    "                        and not len(str(run.data.tags['noise_type'])) < 1 \r\n",
    "                    else '',\r\n",
    "                run_id=run_id,\r\n",
    "            )\r\n",
    "            \r\n",
    "            for other_run in runs:\r\n",
    "                if not 'model_type' in other_run.data.tags:\r\n",
    "                    continue\r\n",
    "                other_run_id = other_run.info.run_id\r\n",
    "                if other_run_id not in relevant_run_ids:\r\n",
    "                    continue\r\n",
    "                other_suffix = '_{model}_{trainable}{noise}_{run_id}'.format(\r\n",
    "                    model=other_run.data.tags['model_type'],\r\n",
    "                    trainable=other_run.data.params['ModelConfigbase_feature_embeddings_trainable'],\r\n",
    "                    noise=(other_run.data.tags['noise_type'] + '_') \r\n",
    "                        if 'noise_type' in run.data.tags \r\n",
    "                            and not str(run.data.tags['noise_type']).lower() == 'nan' \r\n",
    "                            and not len(str(run.data.tags['noise_type'])) < 1 \r\n",
    "                        else '',\r\n",
    "                    run_id=other_run_id,\r\n",
    "                )\r\n",
    "                if suffix in correlation_map and other_suffix in correlation_map[suffix]: \r\n",
    "                    pbar.update(1)\r\n",
    "                    continue\r\n",
    "                if other_suffix in correlation_map and suffix in correlation_map[other_suffix]: \r\n",
    "                    if suffix not in correlation_map:\r\n",
    "                        correlation_map[suffix] = {}\r\n",
    "                    correlation_map[suffix][other_suffix] = correlation_map[other_suffix][suffix]\r\n",
    "                    pbar.update(1)\r\n",
    "                    continue\r\n",
    "                \r\n",
    "                correlation = calculate_gcca_corr_between(run_id, other_run_id, relevant_run_ids, dims=dims, local_mlflow_dir=local_mlflow_dir)\r\n",
    "                pbar.update(1)\r\n",
    "                if correlation is not None:\r\n",
    "                    if suffix not in correlation_map:\r\n",
    "                        correlation_map[suffix] = {}\r\n",
    "                    correlation_map[suffix][other_suffix] = correlation\r\n",
    "                    \r\n",
    "                \r\n",
    "    return correlation_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "runs = mlflow_helper.mlflow_client.search_runs(experiment_ids=[mlflow_helper.experiment.experiment_id], max_results=10000)\r\n",
    "correlations = calculate_all_correlations(runs, set(mlflow_helper.mimic_run_df()['info_run_id']), correlation_map={}, dims=300)\r\n",
    "pretty_print_result_dict(correlations)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "suffix_mapping = {x:\"_\".join(x.split(\"_\")[0:len(x.split(\"_\"))-1]) for x in suffixes}\r\n",
    "for suffix in suffix_mapping:\r\n",
    "    same_mapped_suffixes = [s for s in suffix_mapping if suffix_mapping[s] == suffix_mapping[suffix]]\r\n",
    "    if len(same_mapped_suffixes) > 1:\r\n",
    "        for idx in range(len(same_mapped_suffixes)):\r\n",
    "            suffix_mapping[same_mapped_suffixes[idx]] = suffix_mapping[same_mapped_suffixes[idx]] + '_' + str(idx)\r\n",
    "\r\n",
    "suffix_mapping "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corr_df = pd.DataFrame.from_dict({\r\n",
    "    suffix_mapping[k_from]:{\r\n",
    "        suffix_mapping[k_to]:v[1] for k_to,v in vs.items()\r\n",
    "    } for k_from,vs in cca_correlations.items()\r\n",
    "}, orient='index').sort_index(axis=0).sort_index(axis=1)\r\n",
    "\r\n",
    "plt.figure(figsize=(20,20))\r\n",
    "ax = sns.heatmap(data=corr_df, vmin=0, vmax=1, cmap='Blues', square=True, annot=True)\r\n",
    "ax.figure.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation of Embedding Distances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_embedding_distances(vec_np, meta_df):\r\n",
    "    distances = squareform(pdist(vec_np)) \r\n",
    "    all_words = [\r\n",
    "        str(row['name'])\r\n",
    "        for _,row in sorted(meta_df.iterrows(), key=lambda x: x[0])\r\n",
    "    ]\r\n",
    "    relevant_words = set([word for word in all_words if not (word.endswith('_hidden') or word.endswith('_base'))])\r\n",
    "    return pd.DataFrame(distances, columns=all_words, index=all_words) \\\r\n",
    "        .drop(labels=[x for x in all_words if x not in relevant_words], axis=0) \\\r\n",
    "        .drop(labels=[x for x in all_words if x not in relevant_words], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "local_mlflow_dir = '../gsim01/mlruns/' + experiment.experiment_id + '/'\r\n",
    "distances_df = None\r\n",
    "suffixes = []\r\n",
    "\r\n",
    "for run in tqdm(runs, desc='Calculating embedding distances per run'):\r\n",
    "    run_id = run.info.run_id\r\n",
    "    if run_id not in set(relevant_run_df['info_run_id']):\r\n",
    "        print('Ignoring run {}'.format(run_id))\r\n",
    "        continue\r\n",
    "\r\n",
    "    run_mlflow_dir = Path(local_mlflow_dir + run_id)\r\n",
    "    if not run_mlflow_dir.is_dir():\r\n",
    "        print('Run {} is not in local MlFlow dir'.format(run_id))\r\n",
    "        continue\r\n",
    "\r\n",
    "    run_vec_path = Path(local_mlflow_dir + run_id + '/artifacts/vecs.tsv')\r\n",
    "    run_meta_path = Path(local_mlflow_dir + run_id + '/artifacts/meta.tsv')\r\n",
    "    if not run_vec_path.exists() or not run_meta_path.exists():\r\n",
    "        print('No vecs.tsv file for run {} in local MlFlow dir'.format(run_id))\r\n",
    "        continue\r\n",
    "\r\n",
    "    vec_np = np.genfromtxt(fname=run_vec_path, delimiter=\"\\t\") # shape: (num_features, embedding_dim)\r\n",
    "    meta_df = pd.read_csv(run_meta_path, sep='\\t', header=None, names=['name'])\r\n",
    "\r\n",
    "    suffix = '_' + run.data.tags['model_type'] + '_' + run.data.params['ModelConfigbase_feature_embeddings_trainable'] + '_' + run_id\r\n",
    "    distance_df = calculate_embedding_distances(vec_np, meta_df)\r\n",
    "    if distances_df is None:\r\n",
    "        distances_df = distance_df.copy()\r\n",
    "        print('Initialized distances df with length {}'.format(len(distances_df)))\r\n",
    "    \r\n",
    "    distances_df = pd.merge(distances_df, distance_df, left_index=True, right_index=True, how='inner', suffixes=('',suffix))\r\n",
    "    print('Merged prediction output df, new length {} (dropna: {}, columns: {})'.format(\r\n",
    "        len(distances_df), len(distances_df.dropna()), len(distances_df.columns)\r\n",
    "    ))\r\n",
    "    suffixes.append(suffix)\r\n",
    "\r\n",
    "distances_df.head()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "suffix_mapping = {x:\"_\".join(x.split(\"_\")[0:len(x.split(\"_\"))-1]) for x in suffixes}\r\n",
    "for suffix in suffix_mapping:\r\n",
    "    same_mapped_suffixes = [s for s in suffix_mapping if suffix_mapping[s] == suffix_mapping[suffix]]\r\n",
    "    if len(same_mapped_suffixes) > 1:\r\n",
    "        for idx in range(len(same_mapped_suffixes)):\r\n",
    "            suffix_mapping[same_mapped_suffixes[idx]] = suffix_mapping[same_mapped_suffixes[idx]] + '_' + str(idx)\r\n",
    "\r\n",
    "suffix_mapping "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "distances_df_renamed = distances_df.rename(columns={\r\n",
    "    pre+suf:pre+suffix_mapping[suf] for suf in suffix_mapping for pre in set(distances_df.index)\r\n",
    "})\r\n",
    "print(len(distances_df.columns))\r\n",
    "print(len(distances_df_renamed.columns))\r\n",
    "distances_df_renamed.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_suffixes = list(set(suffix_mapping.values()))\r\n",
    "all_labels = set(distances_df_renamed.index)\r\n",
    "correlation_summary_dfs = []\r\n",
    "for label in tqdm(all_labels, desc=\"Adding correlation summaries per label\"):\r\n",
    "    relevant_columns = [x for x in distances_df_renamed.columns if x.startswith(label)]\r\n",
    "    for method in [\"spearman\", \"pearson\", \"kendall\"]:\r\n",
    "        correlation_df = distances_df_renamed[relevant_columns].corr(method=method)\r\n",
    "        for s1_idx in range(len(all_suffixes)):\r\n",
    "            s1 = all_suffixes[s1_idx]\r\n",
    "            for s2_idx in range(s1_idx+1, len(all_suffixes)):\r\n",
    "                s2 = all_suffixes[s2_idx]\r\n",
    "                correlation_summary_dfs.append(\r\n",
    "                    pd.DataFrame.from_dict({\r\n",
    "                        's1': [s1, s2], \r\n",
    "                        's2': [s2, s1], \r\n",
    "                        's1<>s2': [\"{}<>{}\".format(s1, s2),\"{}<>{}\".format(s2, s1)], \r\n",
    "                        'label': [label, label], \r\n",
    "                        'correlation': [correlation_df.loc[label+s1, label+s2],correlation_df.loc[label+s2, label+s1]],\r\n",
    "                        'method': [method, method],\r\n",
    "                    })\r\n",
    "                )\r\n",
    "\r\n",
    "correlation_summary_df = pd.concat(correlation_summary_dfs, ignore_index=True)\r\n",
    "correlation_summary_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#fig = px.box(correlation_summary_df, facet_col=\"s1\", facet_row=\"s2\", x=\"method\", y=\"correlation\", hover_data=[\"label\"]) \r\n",
    "#fig.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#sns.catplot(data=correlation_summary_df, x=\"method\", y=\"correlation\", row=\"s1\", col=\"s2\", kind='box', height=5, aspect=1)\r\n",
    "#plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_df = correlation_summary_df.groupby(by=[\"method\", \"s1\", \"s2\"], as_index=False).mean()\r\n",
    "grouped_df = grouped_df[grouped_df[\"method\"] == \"pearson\"].pivot(index='s1', columns='s2', values='correlation').fillna(1)\r\n",
    "\r\n",
    "plt.figure(figsize=(20,20))\r\n",
    "#ax = sns.heatmap(data=grouped_df, vmin=-1, vmax=1, center=0, cmap='Blues', square=True)\r\n",
    "ax = sns.heatmap(data=grouped_df, vmax=1, cmap='Blues', square=True, annot=True)\r\n",
    "ax.figure.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}