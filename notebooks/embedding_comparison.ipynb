{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/mlflow_query.py\n",
    "%run utils/loading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_helper = MlflowHelper(pkl_file=Path(\"mlflow_run_df.pkl\"))\n",
    "#mlflow_helper.query_all_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCCA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, views):\n",
    "        dims = [view.shape[1] for view in views]\n",
    "        concat = np.concatenate(views, axis = 1)\n",
    "        self.mean = concat.mean(axis = 0)\n",
    "        cov = np.cov(concat.T)\n",
    "\n",
    "        mask = linalg.block_diag(*[np.ones((dim, dim), bool) for dim in dims])\n",
    "\n",
    "        eigvals, eigvecs = linalg.eigh(cov * np.invert(mask), cov * mask)\n",
    "        tol = abs(eigvals.real).max() * len(eigvals) * np.finfo(float).eps\n",
    "\n",
    "        self.theta = eigvecs.real[:, np.greater(abs(eigvals.real), tol)]\n",
    "    \n",
    "    def transform(self, views):\n",
    "        concat = np.concatenate(views, axis = 1)\n",
    "        return (concat - self.mean).dot(self.theta)\n",
    "\n",
    "    def transform_as_list(self, views):\n",
    "        dims = [view.shape[1] for view in views]\n",
    "        outputs = []\n",
    "\n",
    "        slc = slice(0,0)\n",
    "        for i, dim in enumerate(dims):\n",
    "            slc = slice(slc.stop, slc.stop + dim)\n",
    "            outputs.append((views[i] - self.mean[slc]).dot(self.theta[slc]))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec_dict(run_id, local_mlflow_dir=mlflow_helper.local_mlflow_dir):\n",
    "    run_vec_path = Path(local_mlflow_dir + run_id + '/artifacts/vecs.tsv')\n",
    "    run_meta_path = Path(local_mlflow_dir + run_id + '/artifacts/meta.tsv')\n",
    "    if not run_vec_path.exists() or not run_meta_path.exists():\n",
    "        return None\n",
    "\n",
    "    vec_np = np.genfromtxt(fname=run_vec_path, delimiter=\"\\t\") # shape: (num_features, embedding_dim)\n",
    "    meta_df = pd.read_csv(run_meta_path, sep='\\t', header=None, names=['name'], dtype=str)\n",
    "    return {\n",
    "        meta_df.loc[idx, 'name']:vec_np[idx,:] for idx in range(len(meta_df))\n",
    "    }\n",
    "\n",
    "def calculate_correlation(vectors, other_vectors, dims=100, print_diag=False):\n",
    "    corr_matrix = np.corrcoef(vectors, other_vectors, rowvar=False)\n",
    "    corr_matrix = corr_matrix[dims:, :dims]\n",
    "    diag = np.diag(corr_matrix)\n",
    "    if print_diag: print(diag)\n",
    "    return np.mean(diag)\n",
    "\n",
    "def pretty_print_result_dict(cca_correlation_dict):\n",
    "    for from_corr, to_corr_dict in cca_correlation_dict.items():\n",
    "        print(from_corr + '\\n')\n",
    "        for to_corr, corr_values in to_corr_dict.items():\n",
    "            print('\\t' + to_corr + '\\n\\t -> before:' + str(corr_values[0]) + '\\n\\t -> after:' + str(corr_values[1]) + '\\n\\t -> fitting score:' + str(corr_values[2]))\n",
    "\n",
    "def calculate_cca_corr_between(run_id, other_run_id, relevant_run_ids, dims=100):\n",
    "    if run_id not in relevant_run_ids or other_run_id not in relevant_run_ids:\n",
    "        return None\n",
    "    \n",
    "    run_mlflow_dir = Path(local_mlflow_dir + run_id)\n",
    "    other_run_mlflow_dir = Path(local_mlflow_dir + other_run_id)\n",
    "    if not run_mlflow_dir.is_dir() or not other_run_mlflow_dir.is_dir():\n",
    "        return None\n",
    "\n",
    "    vec_dict = load_vec_dict(run_id)\n",
    "    other_vec_dict = load_vec_dict(other_run_id)\n",
    "    if vec_dict is None or other_vec_dict is None:\n",
    "        return None\n",
    "\n",
    "    shared_meta = [\n",
    "        x for x in vec_dict if x in other_vec_dict and not x.endswith('_hidden') and not x.endswith('_base')\n",
    "    ]\n",
    "\n",
    "    vectors = np.array([vec_dict[x] for x in shared_meta])\n",
    "    other_vectors = np.array([other_vec_dict[x] for x in shared_meta])\n",
    "    corr_before = calculate_correlation(vectors, other_vectors, dims=300)\n",
    "\n",
    "    print('Calculating CCA...')\n",
    "    cca = CCA(n_components=dims, max_iter=5000)\n",
    "    cca.fit(vectors, other_vectors)\n",
    "    vectors_t, other_vectors_t = cca.transform(\n",
    "        vectors, other_vectors\n",
    "    )\n",
    "\n",
    "    print('Calculating correlation coefficient...')\n",
    "    corr_after = calculate_correlation(vectors_t, other_vectors_t, dims=dims, print_diag=True)\n",
    "    return (corr_before, corr_after, cca.score(vectors, other_vectors))\n",
    "\n",
    "def calculate_gcca_corr_between(run_id, other_run_id, relevant_run_ids, dims=100, local_mlflow_dir=mlflow_helper.local_mlflow_dir):\n",
    "    if run_id not in relevant_run_ids or other_run_id not in relevant_run_ids:\n",
    "        return None\n",
    "    \n",
    "    run_mlflow_dir = Path(local_mlflow_dir + run_id)\n",
    "    other_run_mlflow_dir = Path(local_mlflow_dir + other_run_id)\n",
    "    if not run_mlflow_dir.is_dir() or not other_run_mlflow_dir.is_dir():\n",
    "        return None\n",
    "\n",
    "    vec_dict = load_vec_dict(run_id)\n",
    "    other_vec_dict = load_vec_dict(other_run_id)\n",
    "    if vec_dict is None or other_vec_dict is None:\n",
    "        return None\n",
    "\n",
    "    shared_meta = [\n",
    "        x for x in vec_dict if x in other_vec_dict and not x.endswith('_hidden') and not x.endswith('_base')\n",
    "    ]\n",
    "    if len(shared_meta) == 0:\n",
    "        return None\n",
    "\n",
    "    vectors = np.array([vec_dict[x] for x in shared_meta])\n",
    "    other_vectors = np.array([other_vec_dict[x] for x in shared_meta])\n",
    "    corr_before = calculate_correlation(vectors, other_vectors, dims=300)\n",
    "\n",
    "    #print('Calculating GCCA...')\n",
    "    gcca = GCCA()\n",
    "    gcca.fit([vectors, other_vectors])\n",
    "    transform_l = gcca.transform_as_list((vectors, other_vectors))\n",
    "    # gcca computes positive and negative correlations (eigenvalues), sorted in ascending order.\n",
    "    # We are only interested in the positive portion\n",
    "    vectors_t = transform_l[0][:,dims:]\n",
    "    other_vectors_t = transform_l[1][:,dims:]\n",
    "\n",
    "    #print('Calculating correlation coefficient...')\n",
    "    corr_after = calculate_correlation(vectors_t, other_vectors_t, dims=dims)\n",
    "    return (corr_before, corr_after, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_correlations(runs, relevant_run_ids, correlation_map={}, local_mlflow_dir=mlflow_helper.local_mlflow_dir, dims=300):\n",
    "    number_correlations = len(relevant_run_ids) * len(relevant_run_ids)\n",
    "    with tqdm(total=number_correlations, desc='Calculating embedding gcca correlations per run') as pbar:\n",
    "        for run in runs:\n",
    "            if not 'data_tags_model_type' in run:\n",
    "                continue\n",
    "            run_id = run[\"info_run_id\"]\n",
    "            if run_id not in relevant_run_ids:\n",
    "                continue\n",
    "            suffix = '_{model}_{trainable}{noise}_{run_id}'.format(\n",
    "                model=run[\"data_tags_model_type\"],\n",
    "                trainable=run[\"data_params_ModelConfigbase_feature_embeddings_trainable\"],\n",
    "                noise=(run[\"data_tags_noise_type\"] + '_') \n",
    "                    if 'data_tags_noise_type' in run \n",
    "                        and not str(run['data_tags_noise_type']).lower() == 'nan' \n",
    "                        and not len(str(run['data_tags_noise_type'])) < 1 \n",
    "                    else '',\n",
    "                run_id=run[\"info_run_id\"],\n",
    "            )\n",
    "            \n",
    "            for other_run in runs:\n",
    "                if not 'data_tags_model_type' in other_run:\n",
    "                    continue\n",
    "                other_run_id = other_run[\"info_run_id\"]\n",
    "                if other_run_id not in relevant_run_ids:\n",
    "                    continue\n",
    "                other_suffix = '_{model}_{trainable}{noise}_{run_id}'.format(\n",
    "                    model=other_run[\"data_tags_model_type\"],\n",
    "                    trainable=other_run[\"data_params_ModelConfigbase_feature_embeddings_trainable\"],\n",
    "                    noise=(other_run[\"data_tags_noise_type\"] + '_') \n",
    "                        if 'data_tags_noise_type' in other_run \n",
    "                            and not str(other_run['data_tags_noise_type']).lower() == 'nan' \n",
    "                            and not len(str(other_run['data_tags_noise_type'])) < 1 \n",
    "                        else '',\n",
    "                    run_id=other_run[\"info_run_id\"],\n",
    "                )\n",
    "                if suffix in correlation_map and other_suffix in correlation_map[suffix]: \n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                if other_suffix in correlation_map and suffix in correlation_map[other_suffix]: \n",
    "                    if suffix not in correlation_map:\n",
    "                        correlation_map[suffix] = {}\n",
    "                    correlation_map[suffix][other_suffix] = correlation_map[other_suffix][suffix]\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                correlation = calculate_gcca_corr_between(run_id, other_run_id, relevant_run_ids, dims=dims, local_mlflow_dir=local_mlflow_dir)\n",
    "                pbar.update(1)\n",
    "                if correlation is not None:\n",
    "                    if suffix not in correlation_map:\n",
    "                        correlation_map[suffix] = {}\n",
    "                    correlation_map[suffix][other_suffix] = correlation\n",
    "                    \n",
    "                \n",
    "    return correlation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_suffix_mapping(suffixes):\n",
    "    suffix_mapping = {x:\"_\".join(x.split(\"_\")[0:len(x.split(\"_\"))-1]).replace(\"_False\", \"\").replace(\"_True\", \"_trainable\").replace(\"gram\", \"hierarchy\").lower() for x in suffixes}\n",
    "    for suffix in suffix_mapping:\n",
    "        same_mapped_suffixes = [s for s in suffix_mapping if suffix_mapping[s] == suffix_mapping[suffix]]\n",
    "        if len(same_mapped_suffixes) > 1:\n",
    "            for idx in range(len(same_mapped_suffixes)):\n",
    "                suffix_mapping[same_mapped_suffixes[idx]] = suffix_mapping[same_mapped_suffixes[idx]] + '_' + str(idx)\n",
    "\n",
    "    return suffix_mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_df = mlflow_helper.huawei_run_df()\n",
    "runs = [\n",
    "    x for x in mlflow_helper.run_df.to_dict(orient=\"index\").values()\n",
    "    if x[\"info_run_id\"] in set(mimic_df[\"info_run_id\"])\n",
    "]\n",
    "run_suffixes = {\n",
    "    x[\"info_run_id\"]:'_{model}_{trainable}{noise}_{run_id}'.format(\n",
    "        model=x[\"data_tags_model_type\"],\n",
    "        trainable=x[\"data_params_ModelConfigbase_feature_embeddings_trainable\"],\n",
    "        noise=(x[\"data_tags_noise_type\"] + '_') \n",
    "            if 'data_tags_noise_type' in x \n",
    "                and not str(x['data_tags_noise_type']).lower() == 'nan' \n",
    "                and not len(str(x['data_tags_noise_type'])) < 1 \n",
    "            else '',\n",
    "        run_id=x[\"info_run_id\"],\n",
    "    )\n",
    "    for x in runs\n",
    "    if x[\"info_run_id\"] in set(mimic_df[\n",
    "        mimic_df[\"data_tags_model_type\"].apply(lambda x: x in [\"gram\", \"simple\", \"text\", \"causal2\"])\n",
    "    ]['info_run_id'])\n",
    "}\n",
    "correlations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_mapping = calculate_suffix_mapping(run_suffixes.values())\n",
    "relevant_ids = [x.split('_')[-1] for x in suffix_mapping if int(suffix_mapping[x][-1]) < 2]\n",
    "relevant_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = calculate_all_correlations(runs, set(relevant_ids), correlation_map=correlations, dims=300)\n",
    "pretty_print_result_dict(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_mapping = calculate_suffix_mapping(correlations.keys())\n",
    "suffix_mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame.from_dict({\n",
    "    suffix_mapping[k_from]:{\n",
    "        suffix_mapping[k_to]:v[1] for k_to,v in vs.items()\n",
    "    } for k_from,vs in correlations.items()\n",
    "}, orient='index').sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "ax = sns.heatmap(\n",
    "    data=corr_df, \n",
    "    vmin=0, \n",
    "    vmax=1, \n",
    "    cmap='Blues', \n",
    "    square=True, \n",
    "    annot=True)\n",
    "ax.figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = \"_trainable_\"\n",
    "\n",
    "order = [\n",
    "    \"_simple\" + trainable + str(i) for i in range(2)\n",
    "] + [\n",
    "    \"_hierarchy\" + trainable + str(i) for i in range(2)\n",
    "] + [\n",
    "    \"_causal\" + trainable + str(i) for i in range(2)\n",
    "] + [\n",
    "    \"_text\" + trainable + str(i) for i in range(2)\n",
    "]\n",
    "corr_df = pd.DataFrame.from_dict({\n",
    "    suffix_mapping[k_from]:{\n",
    "        suffix_mapping[k_to]:v[1] for k_to,v in vs.items() if \"trainable\" in suffix_mapping[k_to]\n",
    "    } for k_from,vs in correlations.items() if \"trainable\" in suffix_mapping[k_from]\n",
    "}, orient='index').sort_index(axis=0).sort_index(axis=1)\n",
    "corr_df.index = pd.CategoricalIndex(corr_df.index, categories=order)\n",
    "corr_df = corr_df.sort_index()[order]\n",
    "\n",
    "plt.figure()#figsize=(20,20))\n",
    "ax = sns.heatmap(\n",
    "    data=corr_df, \n",
    "    vmin=0, \n",
    "    vmax=1, \n",
    "    cmap='Blues', \n",
    "    square=True, \n",
    "    annot=True,\n",
    "    linecolor='white',\n",
    "    linewidth=1,\n",
    "    cbar_kws={'label': 'Embedding Space Similarity'})\n",
    "ax.figure.tight_layout()\n",
    "#plt.savefig(\"embssim_trainable_all.png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"_causal\"\n",
    "\n",
    "order = [\n",
    "    model_type + \"_\" + str(i) for i in range(2)\n",
    "] + [\n",
    "    model_type + \"_trainable_\" + str(i) for i in range(2)\n",
    "] #+ [\n",
    "  #  \"_simple_trainable_\" + str(i) for i in range(2)\n",
    "#]\n",
    "corr_df = pd.DataFrame.from_dict({\n",
    "    suffix_mapping[k_from]:{\n",
    "        suffix_mapping[k_to]:v[1] for k_to,v in vs.items() if model_type in suffix_mapping[k_to] #or \"_simple_trainable_\" in suffix_mapping[k_to]\n",
    "    } for k_from,vs in correlations.items() if model_type in suffix_mapping[k_from] #or \"_simple_trainable_\" in suffix_mapping[k_from]\n",
    "}, orient='index').sort_index(axis=0).sort_index(axis=1)\n",
    "corr_df.index = pd.CategoricalIndex(corr_df.index, categories=order)\n",
    "corr_df = corr_df.sort_index()[order]\n",
    "\n",
    "plt.figure()#figsize=(20,20))\n",
    "ax = sns.heatmap(\n",
    "    data=corr_df, \n",
    "    vmin=0, \n",
    "    vmax=1, \n",
    "    cmap='Blues', \n",
    "    square=True, \n",
    "    annot=True,\n",
    "    linecolor='white',\n",
    "    linewidth=1,\n",
    "    cbar_kws={'label': 'Embedding Space Similarity'})\n",
    "ax.figure.tight_layout()\n",
    "plt.savefig(\"embssim\" + model_type + \".png\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation of Embedding Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_distances(vec_np, meta_df):\n",
    "    distances = squareform(pdist(vec_np)) \n",
    "    all_words = [\n",
    "        str(row['name'])\n",
    "        for _,row in sorted(meta_df.iterrows(), key=lambda x: x[0])\n",
    "    ]\n",
    "    relevant_words = set([word for word in all_words if not (word.endswith('_hidden') or word.endswith('_base'))])\n",
    "    return pd.DataFrame(distances, columns=all_words, index=all_words) \\\n",
    "        .drop(labels=[x for x in all_words if x not in relevant_words], axis=0) \\\n",
    "        .drop(labels=[x for x in all_words if x not in relevant_words], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_mlflow_dir = '../gsim01/mlruns/' + experiment.experiment_id + '/'\n",
    "distances_df = None\n",
    "suffixes = []\n",
    "\n",
    "for run in tqdm(runs, desc='Calculating embedding distances per run'):\n",
    "    run_id = run.info.run_id\n",
    "    if run_id not in set(relevant_run_df['info_run_id']):\n",
    "        print('Ignoring run {}'.format(run_id))\n",
    "        continue\n",
    "\n",
    "    run_mlflow_dir = Path(local_mlflow_dir + run_id)\n",
    "    if not run_mlflow_dir.is_dir():\n",
    "        print('Run {} is not in local MlFlow dir'.format(run_id))\n",
    "        continue\n",
    "\n",
    "    run_vec_path = Path(local_mlflow_dir + run_id + '/artifacts/vecs.tsv')\n",
    "    run_meta_path = Path(local_mlflow_dir + run_id + '/artifacts/meta.tsv')\n",
    "    if not run_vec_path.exists() or not run_meta_path.exists():\n",
    "        print('No vecs.tsv file for run {} in local MlFlow dir'.format(run_id))\n",
    "        continue\n",
    "\n",
    "    vec_np = np.genfromtxt(fname=run_vec_path, delimiter=\"\\t\") # shape: (num_features, embedding_dim)\n",
    "    meta_df = pd.read_csv(run_meta_path, sep='\\t', header=None, names=['name'])\n",
    "\n",
    "    suffix = '_' + run.data.tags['model_type'] + '_' + run.data.params['ModelConfigbase_feature_embeddings_trainable'] + '_' + run_id\n",
    "    distance_df = calculate_embedding_distances(vec_np, meta_df)\n",
    "    if distances_df is None:\n",
    "        distances_df = distance_df.copy()\n",
    "        print('Initialized distances df with length {}'.format(len(distances_df)))\n",
    "    \n",
    "    distances_df = pd.merge(distances_df, distance_df, left_index=True, right_index=True, how='inner', suffixes=('',suffix))\n",
    "    print('Merged prediction output df, new length {} (dropna: {}, columns: {})'.format(\n",
    "        len(distances_df), len(distances_df.dropna()), len(distances_df.columns)\n",
    "    ))\n",
    "    suffixes.append(suffix)\n",
    "\n",
    "distances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_mapping = {x:\"_\".join(x.split(\"_\")[0:len(x.split(\"_\"))-1]) for x in suffixes}\n",
    "for suffix in suffix_mapping:\n",
    "    same_mapped_suffixes = [s for s in suffix_mapping if suffix_mapping[s] == suffix_mapping[suffix]]\n",
    "    if len(same_mapped_suffixes) > 1:\n",
    "        for idx in range(len(same_mapped_suffixes)):\n",
    "            suffix_mapping[same_mapped_suffixes[idx]] = suffix_mapping[same_mapped_suffixes[idx]] + '_' + str(idx)\n",
    "\n",
    "suffix_mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df_renamed = distances_df.rename(columns={\n",
    "    pre+suf:pre+suffix_mapping[suf] for suf in suffix_mapping for pre in set(distances_df.index)\n",
    "})\n",
    "print(len(distances_df.columns))\n",
    "print(len(distances_df_renamed.columns))\n",
    "distances_df_renamed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_suffixes = list(set(suffix_mapping.values()))\n",
    "all_labels = set(distances_df_renamed.index)\n",
    "correlation_summary_dfs = []\n",
    "for label in tqdm(all_labels, desc=\"Adding correlation summaries per label\"):\n",
    "    relevant_columns = [x for x in distances_df_renamed.columns if x.startswith(label)]\n",
    "    for method in [\"spearman\", \"pearson\", \"kendall\"]:\n",
    "        correlation_df = distances_df_renamed[relevant_columns].corr(method=method)\n",
    "        for s1_idx in range(len(all_suffixes)):\n",
    "            s1 = all_suffixes[s1_idx]\n",
    "            for s2_idx in range(s1_idx+1, len(all_suffixes)):\n",
    "                s2 = all_suffixes[s2_idx]\n",
    "                correlation_summary_dfs.append(\n",
    "                    pd.DataFrame.from_dict({\n",
    "                        's1': [s1, s2], \n",
    "                        's2': [s2, s1], \n",
    "                        's1<>s2': [\"{}<>{}\".format(s1, s2),\"{}<>{}\".format(s2, s1)], \n",
    "                        'label': [label, label], \n",
    "                        'correlation': [correlation_df.loc[label+s1, label+s2],correlation_df.loc[label+s2, label+s1]],\n",
    "                        'method': [method, method],\n",
    "                    })\n",
    "                )\n",
    "\n",
    "correlation_summary_df = pd.concat(correlation_summary_dfs, ignore_index=True)\n",
    "correlation_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = px.box(correlation_summary_df, facet_col=\"s1\", facet_row=\"s2\", x=\"method\", y=\"correlation\", hover_data=[\"label\"]) \n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.catplot(data=correlation_summary_df, x=\"method\", y=\"correlation\", row=\"s1\", col=\"s2\", kind='box', height=5, aspect=1)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = correlation_summary_df.groupby(by=[\"method\", \"s1\", \"s2\"], as_index=False).mean()\n",
    "grouped_df = grouped_df[grouped_df[\"method\"] == \"pearson\"].pivot(index='s1', columns='s2', values='correlation').fillna(1)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "#ax = sns.heatmap(data=grouped_df, vmin=-1, vmax=1, center=0, cmap='Blues', square=True)\n",
    "ax = sns.heatmap(data=grouped_df, vmax=1, cmap='Blues', square=True, annot=True)\n",
    "ax.figure.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d90ef2e2544a65949a5382aa665e8a895142ccb15d506742792c571feba52d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
