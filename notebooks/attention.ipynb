{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/attention_graph.py\n",
    "%run utils/mlflow_query.py\n",
    "%run utils/loading.py\n",
    "%run utils/comparison.py\n",
    "%run utils/ranks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_helper = MlflowHelper(pkl_file=Path(\"mlflow_run_df.pkl\"))\n",
    "#mlflow_helper.query_all_runs(pkl_file=Path(\"mlflow_run_df.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_mimic_run_df = mlflow_helper.mimic_run_df(include_noise=True, include_refinements=False)\n",
    "mimic_gram_false_00_run_id = relevant_mimic_run_df[\n",
    "        (relevant_mimic_run_df['data_tags_noise_type'].fillna('').apply(len) == 0) &   \n",
    "        (relevant_mimic_run_df['data_tags_model_type'] == 'gram') &\n",
    "        (relevant_mimic_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n",
    "mimic_gram_false_10_run_id = relevant_mimic_run_df[\n",
    "        (relevant_mimic_run_df['data_tags_noise_type'].fillna('') == 'added0.1_removed0.0_threshold0.0') &   \n",
    "        (relevant_mimic_run_df['data_tags_model_type'] == 'gram') &\n",
    "        (relevant_mimic_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n",
    "mimic_text_false_00_run_id = relevant_mimic_run_df[\n",
    "        (relevant_mimic_run_df['data_tags_noise_type'].fillna('').apply(len) == 0) &\n",
    "        (relevant_mimic_run_df['data_tags_model_type'] == 'text') &\n",
    "        (relevant_mimic_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n",
    "mimic_text_false_10_run_id = relevant_mimic_run_df[\n",
    "        (relevant_mimic_run_df['data_tags_noise_type'].fillna('') == 'added0.1_removed0.0_threshold0.0') &  \n",
    "        (relevant_mimic_run_df['data_tags_model_type'] == 'text') &\n",
    "        (relevant_mimic_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n",
    "mimic_causal_false_00_run_id = relevant_mimic_run_df[\n",
    "        (relevant_mimic_run_df['data_tags_noise_type'].fillna('').apply(len) == 0) &\n",
    "        (relevant_mimic_run_df['data_tags_model_type'] == 'causal') &\n",
    "        (relevant_mimic_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n",
    "mimic_causal_false_10_run_id = relevant_mimic_run_df[\n",
    "        (relevant_mimic_run_df['data_tags_noise_type'].fillna('') == 'added0.1_removed0.0_threshold0.0') & \n",
    "        (relevant_mimic_run_df['data_tags_model_type'] == 'causal') &\n",
    "        (relevant_mimic_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n",
    "print('Gram', mimic_gram_false_00_run_id, 'Text', mimic_text_false_00_run_id, 'Causal', mimic_causal_false_00_run_id)\n",
    "print('NOISE 10%: Gram', mimic_gram_false_10_run_id, 'Text', mimic_text_false_10_run_id, 'Causal', mimic_causal_false_10_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_huawei_run_df = mlflow_helper.huawei_run_df(include_noise=False, include_refinements=False)\n",
    "huawei_gram_false_00_run_id = relevant_huawei_run_df[\n",
    "        (relevant_huawei_run_df['data_tags_noise_type'].fillna('').apply(len) == 0) &   \n",
    "        (relevant_huawei_run_df['data_tags_model_type'] == 'gram') &\n",
    "        (relevant_huawei_run_df['data_params_ModelConfigbase_hidden_embeddings_trainable'] == 'False')\n",
    "].iloc[0].get('info_run_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph_visualization(\n",
    "    run_id=huawei_gram_false_00_run_id,\n",
    "    local_mlflow_dir=mlflow_helper.local_mlflow_dir,\n",
    "    threshold=0.2,\n",
    "    run_name=\"huawe_gram\",\n",
    "    use_node_mapping=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shared_attention_weights(attention_weights: Dict[str, Dict[str, float]]):\n",
    "    if attention_weights is None:\n",
    "        return [0.0]\n",
    "    attention_importances = calculate_attention_importances(attention_weights)\n",
    "    shared_weights = [\n",
    "        sum([\n",
    "            float(weight) for con_feature, weight in attention_weights[in_feature].items()\n",
    "            if len(attention_importances[con_feature]) > 1    \n",
    "        ])\n",
    "        for in_feature in attention_weights\n",
    "    ]\n",
    "    return shared_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_runs = mlflow_helper.mimic_run_df()\n",
    "shared_weights = []\n",
    "for run_id in set(rel_runs[\"info_run_id\"]):\n",
    "    attention_weights = load_attention_weights(run_id=run_id, local_mlflow_dir=mlflow_helper.local_mlflow_dir)\n",
    "    shared_weights.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"shared_weights\": calculate_shared_attention_weights(attention_weights)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_df = pd.merge(rel_runs, pd.DataFrame.from_records(shared_weights), left_on=\"info_run_id\", right_on=\"run_id\")\n",
    "shared_df[\"avg_shared_weights\"] = shared_df[\"shared_weights\"].apply(lambda x: np.mean(x))\n",
    "shared_df[\"median_shared_weights\"] = shared_df[\"shared_weights\"].apply(lambda x: np.median(x))\n",
    "shared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "shared_df[\"data_tags_model_type\"] = shared_df[\"data_tags_model_type\"].apply(\n",
    "    lambda x: {\n",
    "        \"gram\": \"hierarchy\",\n",
    "        \"causal\": \"causal_old\",\n",
    "        \"causal2\": \"causal\",\n",
    "    }.get(x,x)\n",
    ")\n",
    "shared_df[\"Embeddings Trainable\"] = shared_df[\"data_params_ModelConfigbase_hidden_embeddings_trainable\"]\n",
    "sns.catplot(\n",
    "    data=shared_df[\n",
    "        shared_df[\"data_tags_model_type\"].apply(lambda x: x in [\"hierarchy\", \"text\", \"causal\"])\n",
    "    ].explode(\"shared_weights\"), \n",
    "    x=\"data_tags_model_type\", \n",
    "    y=\"shared_weights\",\n",
    "    hue=\"Embeddings Trainable\",\n",
    "    kind=\"box\",\n",
    "    order=[\"hierarchy\", \"causal\", \"text\"],\n",
    "    palette=\"Set2\",\n",
    ").set_axis_labels(\"\", \"shared attention importance\")\n",
    "plt.savefig(\"sharedimportances_trainable_healthcare.png\", dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "texts = load_icd9_text()\n",
    "unknowns = set([x for x,y in texts.items() if \n",
    "    (y[\"description\"].lower().startswith(\"other\")\n",
    "    or y[\"description\"].lower().startswith(\"unspecified\")\n",
    "    or y[\"description\"].lower().endswith(\"unspecified\")\n",
    "    or y[\"description\"].lower().endswith(\"unspecified type\")\n",
    "    or y[\"description\"].lower().endswith(\"not elsewhere classified\"))])\n",
    "\n",
    "attentions = load_attention_weights(\n",
    "    mimic_gram_false_00_run_id, \n",
    "    mlflow_helper.local_mlflow_dir\n",
    ")\n",
    "print(sum([len(x) for x in attentions.values()]))\n",
    "attentions_without_unknowns = {\n",
    "    x:[y for y in ys if y not in unknowns or x == y] for x,ys in attentions.items()\n",
    "}\n",
    "print(sum([len(x) for x in attentions_without_unknowns.values()]))\n",
    "with open('gram_without_unknowns.json', 'w') as f:\n",
    "    json.dump(attentions_without_unknowns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def transform_to_words(description: str) -> Set[str]:\n",
    "    description = description.translate(\n",
    "        str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    )\n",
    "    words = [str(x).lower().strip() for x in description.split()]\n",
    "    return set([x for x in words if len(x) > 0])\n",
    "\n",
    "input_descriptions = [(x, transform_to_words(y[\"description\"])) for x,y in texts.items() if x in attentions]\n",
    "\n",
    "word_overlaps = {}\n",
    "for x, x_desc in tqdm(input_descriptions):\n",
    "    for y, y_desc in input_descriptions:\n",
    "        if x == y:\n",
    "            continue\n",
    "\n",
    "\n",
    "        word_overlap = x_desc.intersection(y_desc)\n",
    "        if len(word_overlap) == 0:\n",
    "            continue\n",
    "\n",
    "        overlap_string = \" \".join([x for x in sorted(word_overlap)])\n",
    "        if overlap_string not in word_overlaps:\n",
    "            word_overlaps[overlap_string] = set()\n",
    "\n",
    "        word_overlaps[overlap_string].update([x,y])\n",
    "\n",
    "print(len(word_overlaps))\n",
    "print(sum([len(ws) for ws in word_overlaps.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_diff = 0.2\n",
    "max_intersection_diff = 0.25\n",
    "cleaned_word_overlaps = {}\n",
    "replacements = {}\n",
    "for words, features in tqdm(word_overlaps.items()):\n",
    "    found_replacement = False\n",
    "    for other_words, other_features in cleaned_word_overlaps.items():\n",
    "        if (len(other_features) <= (1 + max_size_diff) * len(features) and \n",
    "            len(other_features) >= (1 - max_size_diff) * len(features) and \n",
    "            len(other_features.intersection(features)) >= max_intersection_diff * len(features)):\n",
    "            #print(\"Found replacement\", \n",
    "            #    words, len(features), \n",
    "            #    other_words, len(other_features), \n",
    "            #    len(other_features.intersection(features)))\n",
    "            if other_words not in replacements:\n",
    "                replacements[other_words] = set()\n",
    "            replacements[other_words].add(words)\n",
    "            found_replacement = True\n",
    "            break\n",
    "    \n",
    "    if not found_replacement:\n",
    "        cleaned_word_overlaps[words] = features\n",
    "\n",
    "print(len(cleaned_word_overlaps))\n",
    "print(sum([len(ws) for ws in cleaned_word_overlaps.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_word_overlaps[\"10 any body burn degree involving less of or percent surface than third unspecified with\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_overlaps))\n",
    "print(sum([len(ws) for ws in word_overlaps.values()]))\n",
    "word_overlaps[\"(acute) asthma exacerbation with\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_node_mapping = create_graph_visualization(\n",
    "    run_id=mimic_gram_false_00_run_id, \n",
    "    local_mlflow_dir=mlflow_helper.local_mlflow_dir,\n",
    "    threshold=0.2, \n",
    "    run_name='mimic_gram_false_00', \n",
    "    use_node_mapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_connections, feature_node_mapping = create_graph_visualization_reference(\n",
    "    run_id=mimic_gram_false_10_run_id, \n",
    "    reference_run_id=mimic_gram_false_00_run_id, \n",
    "    local_mlflow_dir=mlflow_helper.local_mlflow_dir,\n",
    "    threshold=0.2, \n",
    "    run_name='mimic_gram_false_00', \n",
    "    use_node_mapping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drain Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huawei_run_df = mlflow_helper.huawei_run_df(include_drain_hierarchy=True)\n",
    "drain_run_id = huawei_run_df[\n",
    "    (huawei_run_df[\"data_params_HuaweiPreprocessorConfigdrain_log_sts\"].fillna(\"[]\").astype(str).apply(len) > 2)\n",
    "    & (huawei_run_df[\"data_tags_model_type\"] == \"gram\")\n",
    "][\"info_run_id\"].iloc[0]\n",
    "drain_run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aw = load_attention_weights(run_id=drain_run_id, local_mlflow_dir=mlflow_helper.local_mlflow_dir)\n",
    "aimp = calculate_attention_importances(aw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drain_clusters = [\n",
    "    (k,[a for a,b in w if float(b) > 0.9]) \n",
    "    for k,w in aimp.items() \n",
    "    if \"log_cluster_template\" in k and k[0].isdigit()]\n",
    "[x for x in drain_clusters if len(x[1]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drain_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drain_levels = [w for k,ws in aw.items() for w in ws if \"log_cluster_template\" in w]\n",
    "drain_levels_ = {}\n",
    "for i in range(3):\n",
    "    drain_levels_[i] = len(set([x for x in drain_levels if str(i) + \"_log_cluster_template\" in x]))\n",
    "\n",
    "drain_levels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_node_mapping = create_graph_visualization(\n",
    "    run_id=drain_run_id, \n",
    "    local_mlflow_dir=mlflow_helper.local_mlflow_dir,\n",
    "    threshold=0.2, \n",
    "    run_name='drain_hierarchy', \n",
    "    use_node_mapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_df = mlflow_helper.mimic_run_df(include_noise=False, include_refinements=False, risk_prediction=False, valid_x_columns=[\"level_0\", \"level_1\", \"level_2\"])\n",
    "mimic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mimic_df.groupby(by=[\"data_params_SequenceConfigx_sequence_column_name\", \"data_tags_model_type\"]).agg({\n",
    "    \"data_metrics_num_connections\": np.mean,\n",
    "    \"data_metrics_x_vocab_size\": np.mean,\n",
    "    \"data_metrics_y_vocab_size\": np.mean,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_hierarchy = pd.read_csv('data/hierarchy_icd9.csv')\n",
    "icd9_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_icd9_hierarchy_parents_for_level(\n",
    "                icd9_hierarchy: pd.DataFrame, \n",
    "                all_features: Set[str], \n",
    "                max_level: str) -> Dict[str, str]:\n",
    "    parent_infos = {}\n",
    "    for feature in tqdm(all_features, desc=\"Processing icd9 hierarchy clusters for level \" + max_level):\n",
    "        parents = set(icd9_hierarchy[icd9_hierarchy[\"level_0\"] == feature][max_level])\n",
    "        if len(parents) > 1:\n",
    "            print(\"Found more than one parent!\", feature, parents)\n",
    "        parent = list(parents)[0]\n",
    "        if feature in parent_infos and parent not in parent_infos[feature]:\n",
    "            print(\"Feature already in weights, but with different parent!\", feature, parent, weights[feature])\n",
    "\n",
    "        parent_infos[feature] = parent\n",
    "\n",
    "    return parent_infos\n",
    "\n",
    "def add_icd9_hierarchy_attention_weights_for_level(\n",
    "                feature_parents: Dict[str, str],\n",
    "                attention_weights: Dict[str, Dict[str, float]]) -> Dict[str, Dict[str, float]]:\n",
    "    new_attention_weights = {}\n",
    "    for feature, parent in feature_parents.items():\n",
    "        if feature in attention_weights:\n",
    "            new_attention_weights[feature] = attention_weights[feature]\n",
    "        elif parent in attention_weights:\n",
    "            new_attention_weights[feature] = attention_weights[parent]\n",
    "        else:\n",
    "            new_attention_weights[feature] = {\n",
    "                parent: 1.0,\n",
    "            }\n",
    "\n",
    "    return new_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_run_id = list(\n",
    "    mimic_df[\n",
    "        (mimic_df[\"data_params_SequenceConfigx_sequence_column_name\"] == \"level_0\") &\n",
    "        (mimic_df[\"data_tags_model_type\"] != \"simple\")\n",
    "    ][\"info_run_id\"]\n",
    ")[0]\n",
    "reference_attention = load_attention_weights(reference_run_id, mlflow_helper.local_mlflow_dir)\n",
    "all_features = set(reference_attention.keys())\n",
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_infos = []\n",
    "\n",
    "for level in set(mimic_df[\"data_params_SequenceConfigx_sequence_column_name\"]):\n",
    "    icd9_parents = load_icd9_hierarchy_parents_for_level(\n",
    "                icd9_hierarchy=icd9_hierarchy, \n",
    "                all_features=all_features, \n",
    "                max_level=level)\n",
    "    \n",
    "    for run_id in set(\n",
    "        mimic_df[\n",
    "            (mimic_df[\"data_params_SequenceConfigx_sequence_column_name\"] == level)\n",
    "        ][\"info_run_id\"]\n",
    "    ):\n",
    "        original_attention = load_attention_weights(run_id, mlflow_helper.local_mlflow_dir)\n",
    "        if original_attention is None:\n",
    "            original_attention = {}\n",
    "        \n",
    "        attention = add_icd9_hierarchy_attention_weights_for_level(\n",
    "                feature_parents=icd9_parents,\n",
    "                attention_weights=original_attention)\n",
    "        attention_importances = calculate_attention_importances(attention)\n",
    "        clusters_around = {\n",
    "            x:[y for y in ys if y[1] > 0.9] for x,ys in attention_importances.items()\n",
    "        }\n",
    "        clusters_around = {\n",
    "            x:ys for x,ys in clusters_around.items() if len(ys) > 0\n",
    "        }\n",
    "        shared_clusters = {\n",
    "            x:ys for x,ys in clusters_around.items() if len(ys) > 1\n",
    "        }\n",
    "        single_clusters = {\n",
    "            x:ys for x,ys in clusters_around.items() if len(ys) == 1\n",
    "        }\n",
    "\n",
    "        all_inputs = set(attention.keys())\n",
    "        clustered_inputs = {\n",
    "            y[0] for _,ys in clusters_around.items() for y in ys\n",
    "        }\n",
    "        shared_clustered_inputs = {\n",
    "            y[0] for _,ys in shared_clusters.items() for y in ys\n",
    "        }\n",
    "        single_clustered_inputs = {\n",
    "            y[0] for _,ys in single_clusters.items() for y in ys\n",
    "        }\n",
    "        non_clustered_inputs = all_inputs - clustered_inputs\n",
    "\n",
    "        if len(original_attention) == 0:\n",
    "            original_attention = {\n",
    "                x:{x:1.0}\n",
    "                for x in icd9_parents.values()\n",
    "            }\n",
    "\n",
    "        attention_importances_o = calculate_attention_importances(original_attention)\n",
    "        clusters_around_o = {\n",
    "            x:[y for y in ys if y[1] > 0.9] for x,ys in attention_importances_o.items()\n",
    "        }\n",
    "        clusters_around_o = {\n",
    "            x:ys for x,ys in clusters_around_o.items() if len(ys) > 0\n",
    "        }\n",
    "        shared_clusters_o = {\n",
    "            x:ys for x,ys in clusters_around_o.items() if len(ys) > 1\n",
    "        }\n",
    "        single_clusters_o = {\n",
    "            x:ys for x,ys in clusters_around_o.items() if len(ys) == 1\n",
    "        }\n",
    "\n",
    "        all_inputs_o = set(original_attention.keys())\n",
    "        clustered_inputs_o = {\n",
    "            y[0] for _,ys in clusters_around_o.items() for y in ys\n",
    "        }\n",
    "        shared_clustered_inputs_o = {\n",
    "            y[0] for _,ys in shared_clusters_o.items() for y in ys\n",
    "        }\n",
    "        single_clustered_inputs_o = {\n",
    "            y[0] for _,ys in single_clusters_o.items() for y in ys\n",
    "        }\n",
    "        non_clustered_inputs_o = all_inputs_o - clustered_inputs_o\n",
    "        cluster_infos.append({\n",
    "            'run_id': run_id, \n",
    "            'all_inputs': len(all_inputs),\n",
    "            'clustered_inputs': len(clustered_inputs),\n",
    "            'clustered_inputs_p': len(clustered_inputs) / len(all_inputs),\n",
    "            'shared_clustered_inputs': len(shared_clustered_inputs),\n",
    "            'shared_clustered_inputs_p': len(shared_clustered_inputs) / len(all_inputs),\n",
    "            'single_clustered_inputs': len(single_clustered_inputs),\n",
    "            'single_clustered_inputs_p': len(single_clustered_inputs) / len(all_inputs),\n",
    "            'non_clustered_inputs': len(non_clustered_inputs),\n",
    "            'non_clustered_inputs_p': len(non_clustered_inputs) / len(all_inputs),\n",
    "            'clusters': len(clusters_around),\n",
    "            'shared_clusters': len(shared_clusters),\n",
    "            'shared_clusters_p': len(shared_clusters) / len(clusters_around),\n",
    "            'single_clusters': len(single_clusters),\n",
    "            'single_clusters_p': len(single_clusters) / len(clusters_around),\n",
    "            'all_inputs_o': len(all_inputs_o),\n",
    "            'clustered_inputs_o': len(clustered_inputs_o),\n",
    "            'clustered_inputs_p_o': len(clustered_inputs_o) / len(all_inputs_o),\n",
    "            'shared_clustered_inputs_o': len(shared_clustered_inputs_o),\n",
    "            'shared_clustered_inputs_p_o': len(shared_clustered_inputs_o) / len(all_inputs_o),\n",
    "            'single_clustered_inputs_o': len(single_clustered_inputs_o),\n",
    "            'single_clustered_inputs_p_o': len(single_clustered_inputs_o) / len(all_inputs_o),\n",
    "            'non_clustered_inputs_o': len(non_clustered_inputs_o),\n",
    "            'non_clustered_inputs_p_o': len(non_clustered_inputs_o) / len(all_inputs_o),\n",
    "            'clusters_o': len(clusters_around_o),\n",
    "            'shared_clusters_o': len(shared_clusters_o),\n",
    "            'shared_clusters_p_o': len(shared_clusters_o) / len(clusters_around_o),\n",
    "            'single_clusters_o': len(single_clusters_o),\n",
    "            'single_clusters_p_o': len(single_clusters_o) / len(clusters_around_o),\n",
    "        })\n",
    "    \n",
    "pd.DataFrame.from_records(cluster_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_columns = cluster_infos[1].keys()\n",
    "merged = pd.merge(\n",
    "    pd.melt(pd.DataFrame.from_records(cluster_infos), id_vars=\"run_id\", value_vars=[x for x in added_columns if x != \"run_id\"]),\n",
    "    mimic_df,\n",
    "    left_on=\"run_id\",\n",
    "    right_on=\"info_run_id\",)\n",
    "merged[[\"variable\", \"value\", \"data_tags_model_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = sns.catplot(\n",
    "    data=merged,\n",
    "    x=\"data_params_SequenceConfigx_sequence_column_name\",\n",
    "    order=[\"level_0\", \"level_1\", \"level_2\"],\n",
    "    sharey=False,\n",
    "    y=\"value\", col=\"variable\", row=\"data_params_ModelConfigbase_hidden_embeddings_trainable\",\n",
    "    kind=\"box\", hue=\"data_tags_model_type\")\n",
    "f.set_titles(\"Trainable: {row_name}, Metric: {col_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sns.catplot(\n",
    "    data=merged[merged[\"variable\"].apply(lambda x: x in [\"clustered_inputs_p\", \"shared_clustered_inputs_p\", \"single_clustered_inputs_p\"])],\n",
    "    x=\"data_params_SequenceConfigx_sequence_column_name\",\n",
    "    order=[\"level_0\", \"level_1\", \"level_2\"],\n",
    "    sharey=False,\n",
    "    y=\"value\", col=\"variable\", row=\"data_params_ModelConfigbase_hidden_embeddings_trainable\",\n",
    "    kind=\"box\", hue=\"data_tags_model_type\")\n",
    "f.set_titles(\"Trainable: {row_name}, Metric: {col_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clusters(run_id, local_mlflow_dir, icd9_parents, threshold=0.9):\n",
    "    original_attention = load_attention_weights(run_id, local_mlflow_dir)\n",
    "    if original_attention is None:\n",
    "        original_attention = {}\n",
    "    \n",
    "    attention = add_icd9_hierarchy_attention_weights_for_level(\n",
    "            feature_parents=icd9_parents,\n",
    "            attention_weights=original_attention)\n",
    "    attention_importances = calculate_attention_importances(attention)\n",
    "    clusters_around = {\n",
    "        x:[y[0] for y in ys if y[1] > threshold] for x,ys in attention_importances.items()\n",
    "    }\n",
    "    clusters_around = {\n",
    "        x:ys for x,ys in clusters_around.items() if len(ys) > 0\n",
    "    }\n",
    "    shared_clusters = {\n",
    "        x:ys for x,ys in clusters_around.items() if len(ys) > 1\n",
    "    }\n",
    "    single_clusters = {\n",
    "        x:ys for x,ys in clusters_around.items() if len(ys) == 1\n",
    "    }\n",
    "\n",
    "    all_inputs = set(attention.keys())\n",
    "    clustered_inputs = {\n",
    "        y for _,ys in clusters_around.items() for y in ys\n",
    "    }\n",
    "    shared_clustered_inputs = {\n",
    "        y for _,ys in shared_clusters.items() for y in ys\n",
    "    }\n",
    "    single_clustered_inputs = {\n",
    "        y for _,ys in single_clusters.items() for y in ys\n",
    "    }\n",
    "    non_clustered_inputs = all_inputs - clustered_inputs\n",
    "    return {\n",
    "        \"clusters_around\": clusters_around,\n",
    "        \"shared_clusters\": shared_clusters,\n",
    "        \"single_clusters\": single_clusters,\n",
    "        \"clustered_inputs\": clustered_inputs,\n",
    "        \"non_clustered_inputs\": non_clustered_inputs,\n",
    "        \"shared_clustered_inputs\": shared_clustered_inputs,\n",
    "        \"single_clustered_inputs\": single_clustered_inputs,\n",
    "    }\n",
    "    \n",
    "\n",
    "def compare_clusters(run_id_1, run_id_2, local_mlflow_dir, icd9_parents_1, icd9_parents_2, cluster_threshold=0.99):\n",
    "    clusters_1 = calculate_clusters(run_id_1, local_mlflow_dir, icd9_parents_1)\n",
    "    clusters_2 = calculate_clusters(run_id_2, local_mlflow_dir, icd9_parents_2)\n",
    "\n",
    "    return {\n",
    "        run_id_1: clusters_1,\n",
    "        run_id_2: clusters_2,\n",
    "        \"same_clustered_inputs\": clusters_1[\"clustered_inputs\"].intersection(clusters_2[\"clustered_inputs\"]),\n",
    "        \"same_nonclustered_inputs\": clusters_1[\"non_clustered_inputs\"].intersection(clusters_2[\"non_clustered_inputs\"]),\n",
    "        \"same_shared_clustered_inputs\": clusters_1[\"shared_clustered_inputs\"].intersection(clusters_2[\"shared_clustered_inputs\"]),\n",
    "        \"same_single_clustered_inputs\": clusters_1[\"single_clustered_inputs\"].intersection(clusters_2[\"single_clustered_inputs\"]),\n",
    "        \"same_clusters\": [\n",
    "            x for x in clusters_1[\"clusters_around\"].values() if len([\n",
    "                y for y in clusters_2[\"clusters_around\"].values() if len(set(y).intersection(set(x))) / len(set(x).union(set(y))) > cluster_threshold\n",
    "            ]) > 0\n",
    "        ],\n",
    "        \"same_shared_clusters\": [\n",
    "            x for x in clusters_1[\"shared_clusters\"].values() if len([\n",
    "                y for y in clusters_2[\"shared_clusters\"].values() if len(set(y).intersection(set(x))) / len(set(x).union(set(y))) > cluster_threshold\n",
    "            ]) > 0\n",
    "        ],\n",
    "        \"same_single_clusters\": [\n",
    "            x for x in clusters_1[\"single_clusters\"].values() if len([\n",
    "                y for y in clusters_2[\"single_clusters\"].values() if len(set(y).intersection(set(x))) / len(set(x).union(set(y))) > cluster_threshold\n",
    "            ]) > 0\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "level_parents = {}\n",
    "\n",
    "for run_id_1 in set(mimic_df[\"info_run_id\"]):\n",
    "    level_1 = mimic_df[mimic_df[\"info_run_id\"] == run_id_1][\"data_params_SequenceConfigx_sequence_column_name\"].iloc[0]\n",
    "    if level_1 not in level_parents:\n",
    "        level_parents[level_1] = load_icd9_hierarchy_parents_for_level(\n",
    "            icd9_hierarchy=icd9_hierarchy, \n",
    "            all_features=all_features, \n",
    "            max_level=level_1)\n",
    "    \n",
    "    icd9_parents_1 = level_parents[level_1]\n",
    "    for run_id_2 in set(mimic_df[\"info_run_id\"]):\n",
    "        level_2 = mimic_df[mimic_df[\"info_run_id\"] == run_id_2][\"data_params_SequenceConfigx_sequence_column_name\"].iloc[0]\n",
    "        if level_2 not in level_parents:\n",
    "            level_parents[level_2] = load_icd9_hierarchy_parents_for_level(\n",
    "                icd9_hierarchy=icd9_hierarchy, \n",
    "                all_features=all_features, \n",
    "                max_level=level_2)\n",
    "        icd9_parents_2 = level_parents[level_2]\n",
    "        comparison = compare_clusters(run_id_1, run_id_2, mlflow_helper.local_mlflow_dir, icd9_parents_1, icd9_parents_2, cluster_threshold=0.9)\n",
    "        comparisons.append({\n",
    "            \"run_id_1\": run_id_1,\n",
    "            \"run_id_2\": run_id_2,\n",
    "            \"same_clusters\": len(comparison[\"same_clusters\"]),\n",
    "            \"same_shared_clusters\": len(comparison[\"same_shared_clusters\"]),\n",
    "            \"same_single_clusters\": len(comparison[\"same_single_clusters\"]),\n",
    "            \"same_clustered_inputs\": len(comparison[\"same_clustered_inputs\"]),\n",
    "            \"same_nonclustered_inputs\": len(comparison[\"same_nonclustered_inputs\"]),\n",
    "            \"same_shared_clustered_inputs\": len(comparison[\"same_shared_clustered_inputs\"]),\n",
    "            \"same_single_clustered_inputs\": len(comparison[\"same_single_clustered_inputs\"]),\n",
    "        })\n",
    "\n",
    "pd.DataFrame.from_records(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1=\"level_2\"\n",
    "level_2 = \"level_0\"\n",
    "comp_1 = \"simple\"\n",
    "comp_2 = \"gram\"\n",
    "\n",
    "icd9_parents_1 = load_icd9_hierarchy_parents_for_level(\n",
    "            icd9_hierarchy=icd9_hierarchy, \n",
    "            all_features=all_features, \n",
    "            max_level=level_1)\n",
    "icd9_parents_2 = load_icd9_hierarchy_parents_for_level(\n",
    "            icd9_hierarchy=icd9_hierarchy, \n",
    "            all_features=all_features, \n",
    "            max_level=level_2)\n",
    "run_id_1 = mimic_df[\n",
    "    (mimic_df[\"data_params_SequenceConfigx_sequence_column_name\"] == level_1) &\n",
    "    (mimic_df[\"data_tags_model_type\"] == comp_1) &\n",
    "    (mimic_df[\"data_params_ModelConfigbase_feature_embeddings_trainable\"] == \"False\")\n",
    "][\"info_run_id\"].iloc[0]\n",
    "run_id_2 = mimic_df[\n",
    "    (mimic_df[\"data_params_SequenceConfigx_sequence_column_name\"] == level_2) &\n",
    "    (mimic_df[\"data_tags_model_type\"] == comp_2) &\n",
    "    (mimic_df[\"data_params_ModelConfigbase_feature_embeddings_trainable\"] == \"False\") &\n",
    "    (mimic_df[\"info_run_id\"] != run_id_1)\n",
    "][\"info_run_id\"].iloc[0]\n",
    "ccomparison = compare_clusters(run_id_1, run_id_2, mlflow_helper.local_mlflow_dir, icd9_parents_1, icd9_parents_2, cluster_threshold=0.9)\n",
    "len(ccomparison[\"same_clusters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ccomparison[\"same_clustered_inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = Comparison(\n",
    "    run_id_1=run_id_1, \n",
    "    suffix_1=\"_\" + comp_1 + level_1, \n",
    "    run_id_2=run_id_2, \n",
    "    suffix_2=\"_\" + comp_2 + level_2, \n",
    "    local_mlflow_dir=mlflow_helper.local_mlflow_dir,\n",
    "    num_percentiles=10,\n",
    "    feature_replacements=icd9_parents_1)\n",
    "plot_rank_comparison(comparison)\n",
    "plot_outlier_distances(comparison)\n",
    "analyse_best_worst_sequences(comparison, num_best_sequences=1, num_worst_sequences=1, descriptions=load_icd9_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank_comparison(comparison, \n",
    "    color=\"avg_input_frequencies_percentile\" + comparison.suffix_1,\n",
    "    hover_data=[\n",
    "        \"avg_input_frequencies_percentile\" + comparison.suffix_1,\n",
    "        \"avg_input_frequencies_percentile\" + comparison.suffix_2,\n",
    "    ])\n",
    "plot_rank_comparison(comparison, \n",
    "    color=\"avg_input_frequencies_percentile\" + comparison.suffix_2,\n",
    "    hover_data=[\n",
    "        \"avg_input_frequencies_percentile\" + comparison.suffix_1,\n",
    "        \"avg_input_frequencies_percentile\" + comparison.suffix_2,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(comparison,\n",
    "    plot_column=\"avg_input_frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=max(comparison.comparison_df.index)\n",
    "\n",
    "display(comparison.comparison_df.loc[index][\"input\" + comparison.suffix_1])\n",
    "display(comparison.comparison_df.loc[index][\"input\" + comparison.suffix_2])\n",
    "display(comparison.comparison_df.loc[index][[\n",
    "    \"output_rank_noties\" + comparison.suffix_1, \n",
    "    \"output_rank_noties\" + comparison.suffix_2, \n",
    "    \"avg_input_frequencies\" + comparison.suffix_1,\n",
    "    \"avg_input_frequencies\" + comparison.suffix_2,\n",
    "    \"outlier_distance\"]])\n",
    "print(comparison.suffix_1)\n",
    "for input in comparison.comparison_df.loc[index][\"original_inputs\" + comparison.suffix_1].split(','):\n",
    "    if input.strip() in comparison.attention_weights_for(comparison.suffix_1):\n",
    "        display(comparison.attention_weights_for(comparison.suffix_1).get(input.strip()))\n",
    "print(comparison.suffix_2)\n",
    "for input in comparison.comparison_df.loc[index][\"original_inputs\" + comparison.suffix_2].split(','):\n",
    "    if input.strip() in comparison.attention_weights_for(comparison.suffix_2):\n",
    "        display(comparison.attention_weights_for(comparison.suffix_2).get(input.strip()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d90ef2e2544a65949a5382aa665e8a895142ccb15d506742792c571feba52d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
