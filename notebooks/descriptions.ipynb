{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd09d90ef2e2544a65949a5382aa665e8a895142ccb15d506742792c571feba52d3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import fasttext.util\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from src.features.preprocessing import ICDDescriptionPreprocessor\n",
    "from src.features.knowledge import DescriptionKnowledge\n",
    "from src.features.sequences import SequenceHandler\n",
    "\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_descriptions = ICDDescriptionPreprocessor('../data/D_ICD_DIAGNOSES.csv').load_descriptions()\n",
    "words = set()\n",
    "for _,row in text_descriptions.iterrows():\n",
    "    text_description = row['description']\n",
    "    text_description = text_description.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    description_words = text_description.split(' ')\n",
    "    description_words = [str(x).lower().strip() for x in description_words]\n",
    "    description_words = [x for x in description_words if len(x) > 0]\n",
    "\n",
    "    words.update(description_words)\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df = pd.DataFrame(data={\n",
    "    'sequence': [\n",
    "        [ # sequence1\n",
    "            ['a', 'b'], # visit1\n",
    "            ['a', 'c'], # visit2\n",
    "        ], \n",
    "        [ # sequence2\n",
    "            ['a', 'b', 'c'],\n",
    "            ['a'],\n",
    "            ['d'],\n",
    "        ],\n",
    "        [ # sequence3\n",
    "            ['a', 'b'], \n",
    "            ['a', 'd'], \n",
    "        ], \n",
    "    ]\n",
    "})\n",
    "description_df = pd.DataFrame(data={\n",
    "    'label': ['a', 'b', 'c', 'd'],\n",
    "    'description': [\n",
    "         'THIS! is apple.....',\n",
    "         'this is: Banana',\n",
    "         'this is COOL* curly fries',\n",
    "         'this iS another,description',\n",
    "    ]\n",
    "})\n",
    "result_description_df = pd.DataFrame(data={\n",
    "    'label': ['a', 'b', 'c', 'd'],\n",
    "    'description': [\n",
    "         ['this', 'is', 'apple'],\n",
    "         ['this', 'is', 'banana'],\n",
    "         ['this', 'is', 'cool', 'curly', 'fries'],\n",
    "         ['this', 'is', 'another', 'description'],\n",
    "    ]\n",
    "})\n",
    "\n",
    "handler = SequenceHandler(flatten=True)\n",
    "split = handler.transform_train_test_split(sequence_df, 'sequence')\n",
    "\n",
    "combined_x = tf.concat([split.train_x, split.test_x], axis=0)\n",
    "combined_y = tf.concat([split.train_y, split.test_y], axis=0)\n",
    "\n",
    "print(combined_x.shape) # (dataset_size, max_length, feature_size)\n",
    "print(combined_y.shape) # (dataset_size, 1, feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_knowledge = DescriptionKnowledge()\n",
    "desc_knowledge.build_knowledge_from_df(description_df, split.vocab)\n",
    "desc_knowledge.descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "model = fasttext.load_model('cc.en.300.bin')\n",
    "model.get_dimension()\n",
    "#fasttext.util.reduce_model(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words - set(model.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_description_length = max([len(x) for x in desc_knowledge.descriptions.values()])\n",
    "max_description_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "concatenated_embeddings = {}\n",
    "\n",
    "pad_vector = tf.constant(0.0, shape=(100,))\n",
    "for idx, description_words in desc_knowledge.descriptions.items():\n",
    "    embeddings[idx] = [\n",
    "        tf.constant(model.get_word_vector(word)) for word in description_words\n",
    "    ] + [\n",
    "        pad_vector for i in range(max_description_length) if i >= len(description_words)\n",
    "    ]\n",
    "\n",
    "    concatenated_embeddings[idx] = tf.stack(\n",
    "        embeddings[idx], axis=0\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "concatenated_embeddings[1].shape # (max_words, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dim = 16\n",
    "kernel_dim = 2\n",
    "stacked = tf.stack([concatenated_embeddings[i] for i in range(len(desc_knowledge.descriptions))], axis=0) # shape: (num_variables, max_words, embedding_dim)\n",
    "layer = tf.keras.layers.Conv1D(filter_dim, kernel_dim, activation='relu', input_shape=(max_description_length, 100))\n",
    "layer2 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "\n",
    "print(layer(stacked).shape) # shape: (num_variables, max_words-kernel_dim + 1, filter_dim)\n",
    "res = layer2(layer(stacked)) # shape: (num_variables, layers, filter_dim), layers = (max_words-kernel_dim + 1 - pool_size + 1) / strides)\n",
    "print(res.shape)\n",
    "\n",
    "embedding_matrix = tf.keras.layers.Flatten()(res) # shape: (num_variables, layers*filter_dim)\n",
    "tf.linalg.matmul(combined_x, embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbedding(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, desc_knowledge, model, max_description_length):\n",
    "        super(MyEmbedding, self).__init__()\n",
    "        self.embeddings = {}\n",
    "        self.concatenated_embeddings = {}\n",
    "        self.desc_knowledge = desc_knowledge\n",
    "\n",
    "        pad_vector = tf.constant(0.0, shape=(100,))\n",
    "        for idx, description_words in desc_knowledge.descriptions.items():\n",
    "            self.embeddings[idx] = [\n",
    "                tf.constant(model.get_word_vector(word)) for word in description_words\n",
    "            ] + [\n",
    "                pad_vector for i in range(max_description_length) if i >= len(description_words)\n",
    "            ]\n",
    "\n",
    "            self.concatenated_embeddings[idx] = tf.stack(\n",
    "                embeddings[idx], axis=0\n",
    "            )\n",
    "\n",
    "        filter_dim = 16\n",
    "        kernel_dim = 2\n",
    "        self.stacked = tf.stack([self.concatenated_embeddings[i] for i in range(len(self.desc_knowledge.descriptions))], axis=0) # shape: (num_variables, max_words, embedding_dim)\n",
    "        \n",
    "        self.conv_layer = tf.keras.layers.Conv1D(filter_dim, kernel_dim, activation='relu', input_shape=(max_description_length, 100))\n",
    "        self.pool_layer = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        self.embedding_matrix = tf.keras.layers.Flatten()(\n",
    "            self.pool_layer(\n",
    "                self.conv_layer(self.stacked))) # shape: (num_variables, layers*filter_dim)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(16)\n",
    "\n",
    "    def call(self, values): # values shape: (dataset_size, max_sequence_length, num_used_nodes)\n",
    "        embedding_representation = tf.linalg.matmul(values, self.embedding_matrix) # shape:\n",
    "        return self.final_layer(embedding_representation) #(dataset_size, max_sequence_length, embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = MyEmbedding(desc_knowledge, model, max_description_length)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(split.max_length, len(split.vocab))),\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.LSTM(100),\n",
    "    tf.keras.layers.Dense(len(split.vocab), activation='relu'),\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    optimizer=tf.optimizers.Adam(), \n",
    "    metrics=['CategoricalAccuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(split.train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(split.train_x, split.train_y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(split.train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 16\n",
    "embeddings = {}\n",
    "for name, idx in desc_knowledge.vocab.items():\n",
    "    embeddings[idx] = tf.Variable(\n",
    "        initial_value=tf.random.normal(shape=(1,embedding_size)),\n",
    "        trainable=True,\n",
    "        name=name,\n",
    "    )\n",
    "            \n",
    "for name, idx in desc_knowledge.words_vocab.items():\n",
    "    embeddings[idx] = tf.Variable(\n",
    "        initial_value=tf.constant(model.get_word_vector(name), shape=(1,model.get_dimension())),\n",
    "        trainable=False,\n",
    "        name=name,\n",
    "    )\n",
    "        \n",
    "concatenated_embeddings = tf.Variable(\n",
    "    tf.expand_dims(\n",
    "        tf.concat(\n",
    "            [embeddings[idx] for idx in range(len(desc_knowledge.vocab))], \n",
    "            axis=0),\n",
    "        1),\n",
    "    trainable=True,\n",
    "    name='concatenated_embeddings',\n",
    ")\n",
    "\n",
    "concatenated_embeddings.shape # (num_variables, 1, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "for idx, words in desc_knowledge.descriptions_set.items():\n",
    "    word_idx = set([desc_knowledge.words_vocab[x] for x in words])\n",
    "    id_neighbour_embeddings = [\n",
    "        embeddings[x]  if (x in word_idx) \n",
    "        else tf.constant(0, shape=(embeddings[len(desc_knowledge.vocab)].shape), dtype='float32')\n",
    "        for x in range(len(desc_knowledge.vocab), len(desc_knowledge.vocab) + len(desc_knowledge.words_vocab))\n",
    "    ]\n",
    "    word_embeddings[idx] = tf.concat(id_neighbour_embeddings, axis=0)\n",
    "\n",
    "all_neighbour_embeddings = [\n",
    "    word_embeddings[idx] for idx in range(len(desc_knowledge.vocab))\n",
    "]\n",
    "concatenated_neighbour_embeddings = tf.Variable(\n",
    "    tf.concat([all_neighbour_embeddings], axis=1),\n",
    "    trainable=True,\n",
    "    name='concatenated_description_embeddings',\n",
    ")\n",
    "\n",
    "concatenated_neighbour_embeddings.shape # (num_variables, num_words, word_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "\n",
    "w1 = tf.keras.layers.Dense(hidden_size)\n",
    "w2 = tf.keras.layers.Dense(hidden_size)\n",
    "u = tf.keras.layers.Dense(1)\n",
    "score = u(tf.nn.tanh(\n",
    "    w1(concatenated_embeddings) + w2(concatenated_neighbour_embeddings)\n",
    ")) # shape: (num_features, num_words, 1)\n",
    "print(score.shape)\n",
    "\n",
    "attention_weights = tf.nn.softmax(score, axis=0) # shape: (num_features, num_words, 1)\n",
    "print(attention_weights.shape)\n",
    "\n",
    "context_vector = attention_weights * concatenated_neighbour_embeddings  # shape: (num_features, num_words, embedding_size)\n",
    "print(context_vector.shape)\n",
    "\n",
    "context_vector = tf.reduce_sum(context_vector, axis=1)  # shape: (num_features, embedding_size)\n",
    "print(context_vector.shape)\n"
   ]
  }
 ]
}