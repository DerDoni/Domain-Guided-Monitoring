{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd09d90ef2e2544a65949a5382aa665e8a895142ccb15d506742792c571feba52d3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import fasttext.util\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "from src.features.preprocessing import ICDDescriptionPreprocessor\n",
    "from src.features.knowledge import DescriptionKnowledge\n",
    "from src.features.sequences import SequenceHandler\n",
    "\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5809"
      ]
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": [
    "text_descriptions = ICDDescriptionPreprocessor('../data/D_ICD_DIAGNOSES.csv').load_descriptions()\n",
    "words = set()\n",
    "for _,row in text_descriptions.iterrows():\n",
    "    text_description = row['description']\n",
    "    text_description = text_description.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    description_words = text_description.split(' ')\n",
    "    description_words = [str(x).lower().strip() for x in description_words]\n",
    "    description_words = [x for x in description_words if len(x) > 0]\n",
    "\n",
    "    words.update(description_words)\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Transforming splitted sequences to tensors: 100%|██████████| 3/3 [00:00<00:00, 2772.79it/s]\n",
      "Transforming splitted sequences to tensors: 100%|██████████| 1/1 [00:00<?, ?it/s](4, 2, 4)\n",
      "(4, 1, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_df = pd.DataFrame(data={\n",
    "    'sequence': [\n",
    "        [ # sequence1\n",
    "            ['a', 'b'], # visit1\n",
    "            ['a', 'c'], # visit2\n",
    "        ], \n",
    "        [ # sequence2\n",
    "            ['a', 'b', 'c'],\n",
    "            ['a'],\n",
    "            ['d'],\n",
    "        ],\n",
    "        [ # sequence3\n",
    "            ['a', 'b'], \n",
    "            ['a', 'd'], \n",
    "        ], \n",
    "    ]\n",
    "})\n",
    "description_df = pd.DataFrame(data={\n",
    "    'label': ['a', 'b', 'c', 'd'],\n",
    "    'description': [\n",
    "         'THIS! is apple.....',\n",
    "         'this is: Banana',\n",
    "         'this is COOL* curly fries',\n",
    "         'this iS another,description',\n",
    "    ]\n",
    "})\n",
    "result_description_df = pd.DataFrame(data={\n",
    "    'label': ['a', 'b', 'c', 'd'],\n",
    "    'description': [\n",
    "         ['this', 'is', 'apple'],\n",
    "         ['this', 'is', 'banana'],\n",
    "         ['this', 'is', 'cool', 'curly', 'fries'],\n",
    "         ['this', 'is', 'another', 'description'],\n",
    "    ]\n",
    "})\n",
    "\n",
    "handler = SequenceHandler(flatten=True)\n",
    "split = handler.transform_train_test_split(sequence_df, 'sequence')\n",
    "\n",
    "combined_x = tf.concat([split.train_x, split.test_x], axis=0)\n",
    "combined_y = tf.concat([split.train_y, split.test_y], axis=0)\n",
    "\n",
    "print(combined_x.shape) # (dataset_size, max_length, feature_size)\n",
    "print(combined_y.shape) # (dataset_size, 1, feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1: ['this', 'is', 'apple'],\n",
       " 3: ['this', 'is', 'banana'],\n",
       " 2: ['this', 'is', 'cool', 'curly', 'fries'],\n",
       " 0: ['this', 'is', 'another', 'description']}"
      ]
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "desc_knowledge = DescriptionKnowledge()\n",
    "desc_knowledge.build_knowledge_from_df(description_df, split.vocab)\n",
    "desc_knowledge.descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x2111a24a9d0>"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "model = fasttext.load_model('cc.en.300.bin')\n",
    "fasttext.util.reduce_model(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "len(words - set(model.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "max_description_length = max([len(x) for x in desc_knowledge.descriptions.values()])\n",
    "max_description_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([5, 100])"
      ]
     },
     "metadata": {},
     "execution_count": 202
    }
   ],
   "source": [
    "embeddings = {}\n",
    "concatenated_embeddings = {}\n",
    "\n",
    "pad_vector = tf.constant(0.0, shape=(100,))\n",
    "for idx, description_words in desc_knowledge.descriptions.items():\n",
    "    embeddings[idx] = [\n",
    "        tf.constant(model.get_word_vector(word)) for word in description_words\n",
    "    ] + [\n",
    "        pad_vector for i in range(max_description_length) if i >= len(description_words)\n",
    "    ]\n",
    "\n",
    "    concatenated_embeddings[idx] = tf.stack(\n",
    "        embeddings[idx], axis=0\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "concatenated_embeddings[1].shape # (max_words, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 4, 16)\n(4, 2, 16)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 32), dtype=float32, numpy=\n",
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.9737123 , 0.41674018, 0.8597754 , 0.03577794, 0.523233  ,\n",
       "         0.42020476, 0.48317412, 0.        , 0.        , 0.9359788 ,\n",
       "         0.3944397 , 0.03953928, 0.        , 0.        , 0.        ,\n",
       "         0.03241515, 0.05803989, 0.12829792, 0.24835162, 0.08965056,\n",
       "         0.05661978, 0.24180092, 0.        , 0.        , 0.18219194,\n",
       "         0.06665231, 0.35654786, 0.24350911, 0.        , 0.10087187,\n",
       "         0.03620609, 0.        ]],\n",
       "\n",
       "       [[0.9737123 , 0.41674018, 0.8597754 , 0.03577794, 0.523233  ,\n",
       "         0.42020476, 0.48317412, 0.        , 0.        , 0.9359788 ,\n",
       "         0.3944397 , 0.03953928, 0.        , 0.        , 0.        ,\n",
       "         0.03241515, 0.05803989, 0.12829792, 0.24835162, 0.08965056,\n",
       "         0.05661978, 0.24180092, 0.        , 0.        , 0.18219194,\n",
       "         0.06665231, 0.35654786, 0.24350911, 0.        , 0.10087187,\n",
       "         0.03620609, 0.        ],\n",
       "        [0.32457078, 0.07552274, 0.20856865, 0.01192598, 0.174411  ,\n",
       "         0.1838228 , 0.16105804, 0.        , 0.        , 0.3119929 ,\n",
       "         0.16286477, 0.03953928, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.02390942, 0.        ,\n",
       "         0.05661978, 0.        , 0.        , 0.        , 0.07055486,\n",
       "         0.        , 0.04081272, 0.17853954, 0.        , 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.64914155, 0.21084416, 0.48552924, 0.02385196, 0.348822  ,\n",
       "         0.38677996, 0.32211608, 0.        , 0.        , 0.6239858 ,\n",
       "         0.22918679, 0.03953928, 0.        , 0.        , 0.        ,\n",
       "         0.03241515, 0.        , 0.        , 0.07550628, 0.        ,\n",
       "         0.05661978, 0.        , 0.        , 0.        , 0.14172453,\n",
       "         0.03390363, 0.07220942, 0.24350911, 0.        , 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.64914155, 0.21084416, 0.48552924, 0.02385196, 0.348822  ,\n",
       "         0.38677996, 0.32211608, 0.        , 0.        , 0.6239858 ,\n",
       "         0.22918679, 0.03953928, 0.        , 0.        , 0.        ,\n",
       "         0.03241515, 0.        , 0.        , 0.07550628, 0.        ,\n",
       "         0.05661978, 0.        , 0.        , 0.        , 0.14172453,\n",
       "         0.03390363, 0.07220942, 0.24350911, 0.        , 0.        ,\n",
       "         0.        , 0.        ]]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "filter_dim = 16\n",
    "kernel_dim = 2\n",
    "stacked = tf.stack([concatenated_embeddings[i] for i in range(len(desc_knowledge.descriptions))], axis=0) # shape: (num_variables, max_words, embedding_dim)\n",
    "layer = tf.keras.layers.Conv1D(filter_dim, kernel_dim, activation='relu', input_shape=(max_description_length, 100))\n",
    "layer2 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "\n",
    "print(layer(stacked).shape) # shape: (num_variables, max_words-kernel_dim + 1, filter_dim)\n",
    "res = layer2(layer(stacked)) # shape: (num_variables, layers, filter_dim), layers = (max_words-kernel_dim + 1 - pool_size + 1) / strides)\n",
    "print(res.shape)\n",
    "\n",
    "embedding_matrix = tf.keras.layers.Flatten()(res) # shape: (num_variables, layers*filter_dim)\n",
    "tf.linalg.matmul(combined_x, embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEmbedding(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, desc_knowledge, model, max_description_length):\n",
    "        super(MyEmbedding, self).__init__()\n",
    "        self.embeddings = {}\n",
    "        self.concatenated_embeddings = {}\n",
    "        self.desc_knowledge = desc_knowledge\n",
    "\n",
    "        pad_vector = tf.constant(0.0, shape=(100,))\n",
    "        for idx, description_words in desc_knowledge.descriptions.items():\n",
    "            self.embeddings[idx] = [\n",
    "                tf.constant(model.get_word_vector(word)) for word in description_words\n",
    "            ] + [\n",
    "                pad_vector for i in range(max_description_length) if i >= len(description_words)\n",
    "            ]\n",
    "\n",
    "            self.concatenated_embeddings[idx] = tf.stack(\n",
    "                embeddings[idx], axis=0\n",
    "            )\n",
    "\n",
    "        filter_dim = 16\n",
    "        kernel_dim = 2\n",
    "        self.stacked = tf.stack([self.concatenated_embeddings[i] for i in range(len(self.desc_knowledge.descriptions))], axis=0) # shape: (num_variables, max_words, embedding_dim)\n",
    "        \n",
    "        self.conv_layer = tf.keras.layers.Conv1D(filter_dim, kernel_dim, activation='relu', input_shape=(max_description_length, 100))\n",
    "        self.pool_layer = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)\n",
    "        self.embedding_matrix = tf.keras.layers.Flatten()(\n",
    "            self.pool_layer(\n",
    "                self.conv_layer(self.stacked))) # shape: (num_variables, layers*filter_dim)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(16)\n",
    "\n",
    "    def call(self, values): # values shape: (dataset_size, max_sequence_length, num_used_nodes)\n",
    "        embedding_representation = tf.linalg.matmul(values, self.embedding_matrix) # shape:\n",
    "        return self.final_layer(embedding_representation) #(dataset_size, max_sequence_length, embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = MyEmbedding(desc_knowledge, model, max_description_length)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(split.max_length, len(split.vocab))),\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.LSTM(100),\n",
    "    tf.keras.layers.Dense(len(split.vocab), activation='relu'),\n",
    "])\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    optimizer=tf.optimizers.Adam(), \n",
    "    metrics=['CategoricalAccuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 16), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.09915152,  0.12963243,  0.04308442, -0.08373883,\n",
       "          0.26392236, -0.15848292,  0.04170059,  1.1119654 ,\n",
       "          0.09878664, -0.42984957, -0.44174978,  0.34599936,\n",
       "         -0.1638078 , -0.4805175 ,  0.80220634, -0.14489104]],\n",
       "\n",
       "       [[ 0.09915152,  0.12963243,  0.04308442, -0.08373883,\n",
       "          0.26392236, -0.15848292,  0.04170059,  1.1119654 ,\n",
       "          0.09878664, -0.42984957, -0.44174978,  0.34599936,\n",
       "         -0.1638078 , -0.4805175 ,  0.80220634, -0.14489104],\n",
       "        [ 0.01124601,  0.02596796, -0.04627941,  0.007903  ,\n",
       "          0.09927685,  0.02314363,  0.0145088 ,  0.41476563,\n",
       "         -0.05339865, -0.1507785 , -0.21187967,  0.05877149,\n",
       "         -0.08967217, -0.15320729,  0.24692604, -0.04724052]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.00307687,  0.04872337, -0.00984534,  0.01776949,\n",
       "          0.15057813,  0.04793319,  0.07655887,  0.7817623 ,\n",
       "         -0.04797476, -0.29183573, -0.4299821 ,  0.14718689,\n",
       "         -0.14191361, -0.32684332,  0.48016953, -0.12306421]]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "embedding_layer(split.train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1d_84/kernel:0', 'conv1d_84/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1d_84/kernel:0', 'conv1d_84/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1d_84/kernel:0', 'conv1d_84/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv1d_84/kernel:0', 'conv1d_84/bias:0'] when minimizing the loss.\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0626 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 3.1872 - categorical_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0931 - categorical_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 3.0428 - categorical_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0092 - categorical_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9842 - categorical_accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9644 - categorical_accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9481 - categorical_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9344 - categorical_accuracy: 0.3333\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9225 - categorical_accuracy: 0.3333\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9121 - categorical_accuracy: 0.3333\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.9028 - categorical_accuracy: 0.3333\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8945 - categorical_accuracy: 0.3333\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8869 - categorical_accuracy: 0.3333\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8799 - categorical_accuracy: 0.3333\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8735 - categorical_accuracy: 0.3333\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8676 - categorical_accuracy: 0.3333\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8621 - categorical_accuracy: 0.3333\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.8569 - categorical_accuracy: 0.3333\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.8520 - categorical_accuracy: 0.3333\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8474 - categorical_accuracy: 0.3333\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8430 - categorical_accuracy: 0.3333\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.8389 - categorical_accuracy: 0.3333\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8349 - categorical_accuracy: 0.3333\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8311 - categorical_accuracy: 0.3333\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8275 - categorical_accuracy: 0.3333\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.8239 - categorical_accuracy: 0.3333\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8206 - categorical_accuracy: 0.3333\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8173 - categorical_accuracy: 0.3333\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.8141 - categorical_accuracy: 0.3333\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.8111 - categorical_accuracy: 0.3333\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8081 - categorical_accuracy: 0.3333\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8052 - categorical_accuracy: 0.3333\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.8024 - categorical_accuracy: 0.3333\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 672us/step - loss: 2.7996 - categorical_accuracy: 0.3333\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7970 - categorical_accuracy: 0.3333\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7943 - categorical_accuracy: 0.3333\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7918 - categorical_accuracy: 0.3333\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7893 - categorical_accuracy: 0.3333\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7869 - categorical_accuracy: 0.3333\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7845 - categorical_accuracy: 0.3333\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7822 - categorical_accuracy: 0.3333\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7800 - categorical_accuracy: 0.3333\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7778 - categorical_accuracy: 0.3333\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7757 - categorical_accuracy: 0.3333\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7736 - categorical_accuracy: 0.3333\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7716 - categorical_accuracy: 0.3333\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7696 - categorical_accuracy: 0.3333\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7677 - categorical_accuracy: 0.3333\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7659 - categorical_accuracy: 0.3333\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7642 - categorical_accuracy: 0.3333\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7625 - categorical_accuracy: 0.3333\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7608 - categorical_accuracy: 0.3333\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7593 - categorical_accuracy: 0.3333\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7578 - categorical_accuracy: 0.3333\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7564 - categorical_accuracy: 0.3333\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7551 - categorical_accuracy: 0.3333\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7539 - categorical_accuracy: 0.3333\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7527 - categorical_accuracy: 0.3333\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7517 - categorical_accuracy: 0.3333\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7507 - categorical_accuracy: 0.3333\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7498 - categorical_accuracy: 0.3333\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7490 - categorical_accuracy: 0.3333\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7483 - categorical_accuracy: 0.3333\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7477 - categorical_accuracy: 0.3333\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7472 - categorical_accuracy: 0.3333\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7468 - categorical_accuracy: 0.3333\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7464 - categorical_accuracy: 0.3333\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7462 - categorical_accuracy: 0.3333\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7459 - categorical_accuracy: 0.3333\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7458 - categorical_accuracy: 0.3333\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.7457 - categorical_accuracy: 0.3333\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7456 - categorical_accuracy: 0.3333\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7455 - categorical_accuracy: 0.3333\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7454 - categorical_accuracy: 0.3333\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 2.7453 - categorical_accuracy: 0.3333\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.7452 - categorical_accuracy: 0.3333\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7451 - categorical_accuracy: 0.3333\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7450 - categorical_accuracy: 0.3333\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7448 - categorical_accuracy: 0.3333\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7446 - categorical_accuracy: 0.3333\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7445 - categorical_accuracy: 0.3333\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7443 - categorical_accuracy: 0.3333\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7441 - categorical_accuracy: 0.3333\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7439 - categorical_accuracy: 0.3333\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7437 - categorical_accuracy: 0.3333\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7435 - categorical_accuracy: 0.3333\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7433 - categorical_accuracy: 0.3333\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7431 - categorical_accuracy: 0.3333\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7430 - categorical_accuracy: 0.3333\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7428 - categorical_accuracy: 0.3333\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7427 - categorical_accuracy: 0.3333\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7425 - categorical_accuracy: 0.3333\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7424 - categorical_accuracy: 0.3333\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 2.7423 - categorical_accuracy: 0.3333\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 2.7421 - categorical_accuracy: 0.3333\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7420 - categorical_accuracy: 0.3333\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7419 - categorical_accuracy: 0.3333\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 2.7418 - categorical_accuracy: 0.3333\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7417 - categorical_accuracy: 0.3333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x211377ed460>"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "model.fit(split.train_x, split.train_y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 16), dtype=float32, numpy=\n",
       "array([[[ 0.02547146,  0.06282518,  0.04444942,  0.00735468,\n",
       "          0.0177834 , -0.04009539,  0.02137122,  0.02424395,\n",
       "          0.01750123, -0.04729794,  0.00691855,  0.06655074,\n",
       "         -0.0437462 ,  0.00925681,  0.02784125, -0.02378744],\n",
       "        [ 0.29274797,  0.5790432 ,  0.33464086, -0.02503546,\n",
       "          0.33848804, -0.48976815,  0.18594624,  1.266968  ,\n",
       "          0.22155604, -0.7139135 , -0.38516307,  0.79723334,\n",
       "         -0.46084324, -0.40556607,  0.99010473, -0.2537799 ]],\n",
       "\n",
       "       [[ 0.29274797,  0.5790432 ,  0.33464086, -0.02503546,\n",
       "          0.33848804, -0.48976815,  0.18594624,  1.266968  ,\n",
       "          0.22155604, -0.7139135 , -0.38516307,  0.79723334,\n",
       "         -0.46084324, -0.40556607,  0.99010473, -0.2537799 ],\n",
       "        [ 0.07831022,  0.18636012,  0.06073102,  0.02788023,\n",
       "          0.13332018, -0.08745211,  0.06674856,  0.47177342,\n",
       "         -0.00957186, -0.25893375, -0.19288921,  0.22399798,\n",
       "         -0.19748554, -0.12776287,  0.31475994, -0.09338856]],\n",
       "\n",
       "       [[ 0.02547146,  0.06282518,  0.04444942,  0.00735468,\n",
       "          0.0177834 , -0.04009539,  0.02137122,  0.02424395,\n",
       "          0.01750123, -0.04729794,  0.00691855,  0.06655074,\n",
       "         -0.0437462 ,  0.00925681,  0.02784125, -0.02378744],\n",
       "        [ 0.12161057,  0.32654697,  0.17490403,  0.05291511,\n",
       "          0.20366083, -0.15059972,  0.16635205,  0.8785535 ,\n",
       "          0.02780439, -0.472963  , -0.39650318,  0.43018726,\n",
       "         -0.3284453 , -0.28186122,  0.59683543, -0.19683413]]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 210
    }
   ],
   "source": [
    "embedding_layer(split.train_x)"
   ]
  }
 ]
}